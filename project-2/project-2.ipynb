{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "paperback-switch",
   "metadata": {},
   "source": [
    "# PROJECT 2: What kind of wine is this?\n",
    "\n",
    "### Deliverables\n",
    "1. Using the data available at `s3://ling583/wine-train.parquet` and `s3://ling583/wine-test.parquet`, construct a classifier that can predict wine variety labels on the basis of review texts. Try out different methods and see what works best. Evaluate your best model using the test data.\n",
    "\n",
    "2. Find the words that your model is using to predict labels (either by looking at the model coefficients or by using a tool like LIME). What aspects of review texts is your model most sensitive to? Is there evidence of overfitting?\n",
    "\n",
    "3. For Reuters texts, we found we could greatly increase the F1 score/accuracy by excluding items that that the model was most uncertain about. How many test examples would we have to exclude to achieve better than 0.85 F1 for this task?\n",
    "\n",
    "4. Another way to improve accuracy is to change the labels. Use a confusion matrix to examine the patterns errors and propose a new labeling scheme. For example, if the model consistently labels “merlot” as “riesling” and vice versa, you might want to create a new label “merlot/riesling”. Is it possible to get better than 0.85 F1 using your classifier trained on a different set of labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "charitable-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-florida",
   "metadata": {},
   "source": [
    "## 1. Contruct a classifier to predict win variety labels \n",
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "certain-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\n",
    "    \"s3://ling583/wine-train.parquet\", storage_options={\"anon\": True}\n",
    ")\n",
    "test = pd.read_parquet(\n",
    "    \"s3://ling583/wine-test.parquet\", storage_options={\"anon\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "detailed-roman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>wine_variant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rich smoky dark cherry nose very intense fruit...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Had this at Corton Restaurant in NYC. First of...</td>\n",
       "      <td>Syrah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nose is very tart, with a layer of sweet fruit...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beautiful golden color. Discrete perfumed nose...</td>\n",
       "      <td>Chardonnay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please take the time to decant: you will not b...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text wine_variant\n",
       "0  Rich smoky dark cherry nose very intense fruit...   Pinot Noir\n",
       "1  Had this at Corton Restaurant in NYC. First of...        Syrah\n",
       "2  Nose is very tart, with a layer of sweet fruit...   Pinot Noir\n",
       "3  Beautiful golden color. Discrete perfumed nose...   Chardonnay\n",
       "4  Please take the time to decant: you will not b...   Pinot Noir"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View a first few observations\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hungry-tunnel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>wine_variant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hmmm. i have mixed emotions about this wine. o...</td>\n",
       "      <td>Chardonnay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You can find several dozen SB Syrahs that have...</td>\n",
       "      <td>Syrah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It has a nice even red finish and remote cherr...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WIML91  Tasted October 6, 2009.  Opened and se...</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My last bottle with the super bowl. Sipped wit...</td>\n",
       "      <td>Zinfandel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text wine_variant\n",
       "0  hmmm. i have mixed emotions about this wine. o...   Chardonnay\n",
       "1  You can find several dozen SB Syrahs that have...        Syrah\n",
       "2  It has a nice even red finish and remote cherr...   Pinot Noir\n",
       "3  WIML91  Tasted October 6, 2009.  Opened and se...   Pinot Noir\n",
       "4  My last bottle with the super bowl. Sipped wit...    Zinfandel"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excited-gibraltar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130497"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "municipal-trial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-revolution",
   "metadata": {},
   "source": [
    "##### Tokenize review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "planned-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\",\n",
    "    exclude=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "dangerous-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp.tokenizer(text)\n",
    "    return [t.norm_ for t in doc if t.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "constitutional-price",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b3ba64fcff4dc9b6e5cccd7a2d88a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89429390fdb547eb91d7543592a58886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"tokens\"] = train[\"review_text\"].progress_apply(tokenize)\n",
    "test[\"tokens\"] = test[\"review_text\"].progress_apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "thousand-tissue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pinot Noir            38471\n",
       "Cabernet Sauvignon    30234\n",
       "Chardonnay            19443\n",
       "Syrah                 13704\n",
       "Riesling               9683\n",
       "Zinfandel              8327\n",
       "Merlot                 5522\n",
       "Sauvignon Blanc        5113\n",
       "Name: wine_variant, dtype: int64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"wine_variant\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-trademark",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Baseline Dummy Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "romance-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all necessary modules\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "considerable-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=5432) # 5-fold cross validation method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quantitative-hanging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29482759, 0.29478927, 0.29480057, 0.29480057, 0.29480057])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = DummyClassifier()\n",
    "cross_val_score(baseline, train[\"tokens\"], train[\"wine_variant\"], cv=cv) # accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stainless-evolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.00      0.00      0.00     30234\n",
      "        Chardonnay       0.00      0.00      0.00     19443\n",
      "            Merlot       0.00      0.00      0.00      5522\n",
      "        Pinot Noir       0.29      1.00      0.46     38471\n",
      "          Riesling       0.00      0.00      0.00      9683\n",
      "   Sauvignon Blanc       0.00      0.00      0.00      5113\n",
      "             Syrah       0.00      0.00      0.00     13704\n",
      "         Zinfandel       0.00      0.00      0.00      8327\n",
      "\n",
      "          accuracy                           0.29    130497\n",
      "         macro avg       0.04      0.12      0.06    130497\n",
      "      weighted avg       0.09      0.29      0.13    130497\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predicted = cross_val_predict(baseline, train[\"tokens\"], train[\"wine_variant\"], cv=cv)\n",
    "print(classification_report(train[\"wine_variant\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-organ",
   "metadata": {},
   "source": [
    "Using the dummy classifier, the model doesn't look really good. It has an F1 accuracy score of 0.29 and a marco average of all the factors of 0.06, which is really low. We need to try a different model. \n",
    "\n",
    "--- \n",
    "\n",
    "### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "underlying-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "composite-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline \n",
    "bnb = make_pipeline(CountVectorizer(analyzer=identity), BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "smaller-panel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.60      0.84      0.70     30234\n",
      "        Chardonnay       0.72      0.89      0.79     19443\n",
      "            Merlot       0.70      0.03      0.05      5522\n",
      "        Pinot Noir       0.75      0.85      0.80     38471\n",
      "          Riesling       0.89      0.62      0.73      9683\n",
      "   Sauvignon Blanc       0.93      0.37      0.54      5113\n",
      "             Syrah       0.73      0.43      0.54     13704\n",
      "         Zinfandel       0.86      0.39      0.53      8327\n",
      "\n",
      "          accuracy                           0.71    130497\n",
      "         macro avg       0.77      0.55      0.59    130497\n",
      "      weighted avg       0.73      0.71      0.68    130497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = cross_val_predict(bnb, train[\"tokens\"], train[\"wine_variant\"], cv=cv, n_jobs=-1)\n",
    "print(classification_report(train[\"wine_variant\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-condition",
   "metadata": {},
   "source": [
    "This method Bernoulli Naive Bayes seems to have better fit results compared to dummay classifier. With a macro average score of F1 of .59, the model has a really balance score between each factor. However, let's try different models to see if we can find better ones.\n",
    "\n",
    "---\n",
    "\n",
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "level-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "romantic-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline for multinomial naive bayes \n",
    "mnb = make_pipeline(CountVectorizer(analyzer=identity), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "structured-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=5432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "several-probe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.61      0.85      0.71     30234\n",
      "        Chardonnay       0.80      0.86      0.83     19443\n",
      "            Merlot       0.82      0.08      0.14      5522\n",
      "        Pinot Noir       0.76      0.85      0.80     38471\n",
      "          Riesling       0.85      0.72      0.78      9683\n",
      "   Sauvignon Blanc       0.90      0.48      0.63      5113\n",
      "             Syrah       0.72      0.48      0.58     13704\n",
      "         Zinfandel       0.83      0.43      0.57      8327\n",
      "\n",
      "          accuracy                           0.73    130497\n",
      "         macro avg       0.79      0.59      0.63    130497\n",
      "      weighted avg       0.75      0.73      0.71    130497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = cross_val_predict(mnb, train[\"tokens\"], train[\"wine_variant\"], cv=cv, n_jobs=-1)\n",
    "print(classification_report(train[\"wine_variant\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-dover",
   "metadata": {},
   "source": [
    "Fortunately, this model mutinomial Naive Bayes has better output compared to the binomial Naive Bayes. This model has an F1 accuracy score of .73, which is really high so far. In addition, the macro average score for F1 is also higher in this model compared to the binomial model. Therefore, we now assume that the Mutinomial Naive Bayes model would give us the best predictions. However, let's still try other models to see if we can find a better model. \n",
    "\n",
    "--- \n",
    "\n",
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "analyzed-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sound-present",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.71      0.79      0.75      7558\n",
      "        Chardonnay       0.82      0.85      0.83      4861\n",
      "            Merlot       0.62      0.36      0.45      1381\n",
      "        Pinot Noir       0.73      0.89      0.80      9618\n",
      "          Riesling       0.80      0.78      0.79      2421\n",
      "   Sauvignon Blanc       0.91      0.61      0.73      1278\n",
      "             Syrah       0.79      0.50      0.61      3426\n",
      "         Zinfandel       0.85      0.53      0.65      2082\n",
      "\n",
      "          accuracy                           0.75     32625\n",
      "         macro avg       0.78      0.66      0.70     32625\n",
      "      weighted avg       0.76      0.75      0.75     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = make_pipeline(CountVectorizer(analyzer=identity), SGDClassifier())\n",
    "sgd.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "predicted = sgd.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"wine_variant\"], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-values",
   "metadata": {},
   "source": [
    "Using a SGD Classifier, the output for the model is looking the best so far compared with the dummy classifier, binomial Naive Bayes, and multinomial Naive Bayes models. This model has an accuracy score of .75 and a macro average of .70 which is quite high so far. We will consider this model to be the best for now. But, let's try different methods to find better models. \n",
    "\n",
    "--- \n",
    "### Tfidf Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "alpha-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "signed-mileage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.69      0.81      0.75      7558\n",
      "        Chardonnay       0.78      0.87      0.82      4861\n",
      "            Merlot       0.87      0.33      0.47      1381\n",
      "        Pinot Noir       0.74      0.88      0.80      9618\n",
      "          Riesling       0.82      0.71      0.76      2421\n",
      "   Sauvignon Blanc       0.84      0.62      0.72      1278\n",
      "             Syrah       0.77      0.49      0.60      3426\n",
      "         Zinfandel       0.89      0.48      0.63      2082\n",
      "\n",
      "          accuracy                           0.75     32625\n",
      "         macro avg       0.80      0.65      0.69     32625\n",
      "      weighted avg       0.76      0.75      0.74     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_idf = make_pipeline(CountVectorizer(analyzer=identity), \n",
    "                        TfidfTransformer(), \n",
    "                        SGDClassifier())\n",
    "sgd_idf.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "predicted_idf = sgd_idf.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"wine_variant\"], predicted_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-maryland",
   "metadata": {},
   "source": [
    "Unfortunately, after running the TFIDF Tranformer, the output of the model doesn't look as good as the SGD Classifier with lower accuracy scores and lower macro average scores. However, the results for this TFIDF Tranformer model is very similar to SGG Classifier by itself. Therefore, it might be worth while to explore the TFIDF and SGD Classifier model. \n",
    "\n",
    "--- \n",
    "### Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "average-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import random as sparse_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "higher-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = make_pipeline(CountVectorizer(analyzer=identity), \n",
    "                    TfidfTransformer(), \n",
    "                    TruncatedSVD(n_components=100),\n",
    "                    SGDClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "designed-green",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.56      0.79      0.65      7558\n",
      "        Chardonnay       0.58      0.83      0.69      4861\n",
      "            Merlot       0.06      0.01      0.01      1381\n",
      "        Pinot Noir       0.67      0.82      0.74      9618\n",
      "          Riesling       0.67      0.34      0.45      2421\n",
      "   Sauvignon Blanc       0.66      0.20      0.31      1278\n",
      "             Syrah       0.61      0.10      0.17      3426\n",
      "         Zinfandel       0.63      0.29      0.40      2082\n",
      "\n",
      "          accuracy                           0.61     32625\n",
      "         macro avg       0.56      0.42      0.43     32625\n",
      "      weighted avg       0.60      0.61      0.56     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svd.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "predicted_svd = svd.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"wine_variant\"], predicted_svd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-calvin",
   "metadata": {},
   "source": [
    "After looking at the results of the Truncated SVD, the model doesn't seem to be better than the SGD Classifier with a lower accuracy score of .61 and a lower macro average of .42. Therefore, we will still stick with the SGD Classifier to be the best model. Again, our SGD Classifier, now, has an F1 score accuracy of 0.75 and and F1 macro average of 0.70. In addition, the weighted average for SGD Classifier is .75 which is the largest number of correctly labelling.  \n",
    "\n",
    "Now that we have tried several models and have found our best model type, SGD Classifier. However, the results of the regression still need some improvement. We will do hyperparameter to search for the best parameters for our classifier.  \n",
    "\n",
    "--- \n",
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "proof-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "from logger import log_search\n",
    "from scipy.stats.distributions import loguniform, randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "downtown-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "flush-florida",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:34055</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.62 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:34055' processes=4 threads=4, memory=16.62 GB>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:34055\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "certain-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"project-2\")\n",
    "sgd = make_pipeline(\n",
    "    CountVectorizer(analyzer=identity), TfidfTransformer(), SGDClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "loaded-filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.68      0.83      0.75      7558\n",
      "        Chardonnay       0.82      0.85      0.83      4861\n",
      "            Merlot       0.68      0.35      0.46      1381\n",
      "        Pinot Noir       0.81      0.83      0.82      9618\n",
      "          Riesling       0.76      0.82      0.79      2421\n",
      "   Sauvignon Blanc       0.85      0.63      0.73      1278\n",
      "             Syrah       0.67      0.58      0.62      3426\n",
      "         Zinfandel       0.80      0.54      0.65      2082\n",
      "\n",
      "          accuracy                           0.76     32625\n",
      "         macro avg       0.76      0.68      0.71     32625\n",
      "      weighted avg       0.76      0.76      0.75     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define base model first\n",
    "sgd_base = make_pipeline(CountVectorizer(analyzer=identity), SGDClassifier())\n",
    "sgd_base.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "predicted_base = sgd_base.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"wine_variant\"], predicted_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "underlying-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 1.43 s, total: 12 s\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    sgd,\n",
    "    {\n",
    "        \"countvectorizer__min_df\": randint(1, 20),\n",
    "        \"countvectorizer__max_df\": uniform(0.5, 0.5),\n",
    "        \"tfidftransformer__use_idf\": [True, False],\n",
    "        \"sgdclassifier__alpha\": loguniform(1e-6, 1e-2),\n",
    "    },\n",
    "    n_iter=50,\n",
    "    scoring=\"f1_macro\",\n",
    ")\n",
    "search.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "log_search(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-howard",
   "metadata": {},
   "source": [
    "We can inputting different parameters to see if we can get better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "excess-bearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.94 s, sys: 1.35 s, total: 11.3 s\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    sgd,\n",
    "    {\n",
    "        \"countvectorizer__min_df\": randint(1, 10),\n",
    "        \"countvectorizer__max_df\": uniform(0.5, 0.5),\n",
    "        \"tfidftransformer__use_idf\": [True, False],\n",
    "        \"sgdclassifier__alpha\": loguniform(1e-8, 100),\n",
    "    },\n",
    "    n_iter=25,\n",
    "    scoring=\"f1_macro\",\n",
    ")\n",
    "search.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "log_search(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "satisfied-suspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.82 s, sys: 1.24 s, total: 11.1 s\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    sgd,\n",
    "    {\n",
    "        \"countvectorizer__min_df\": randint(1, 10),\n",
    "        \"countvectorizer__max_df\": uniform(0.5, 0.5),\n",
    "        \"tfidftransformer__use_idf\": [True, False],\n",
    "        \"sgdclassifier__alpha\": [0.1],\n",
    "    },\n",
    "    n_iter=25,\n",
    "    scoring=\"f1_macro\",\n",
    ")\n",
    "search.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "log_search(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-rendering",
   "metadata": {},
   "source": [
    "### Compare optimized model to baseline\n",
    "\n",
    "Getting results from MLflow that I have created. The best parameter with the best mean score of 0.71 have the following parameters: \n",
    "- `countvectorizer_min_df`: 1\n",
    "- `countvectorizer_max_df`: 0.92\n",
    "- `tfidftransformer_use_idf`: True\n",
    "- `sgdclassifier__alpha`: 7e-06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "amended-mumbai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.70      0.81      0.75      7558\n",
      "        Chardonnay       0.83      0.85      0.84      4861\n",
      "            Merlot       0.82      0.34      0.48      1381\n",
      "        Pinot Noir       0.77      0.87      0.82      9618\n",
      "          Riesling       0.79      0.81      0.80      2421\n",
      "   Sauvignon Blanc       0.84      0.66      0.74      1278\n",
      "             Syrah       0.74      0.55      0.63      3426\n",
      "         Zinfandel       0.80      0.54      0.65      2082\n",
      "\n",
      "          accuracy                           0.76     32625\n",
      "         macro avg       0.79      0.68      0.71     32625\n",
      "      weighted avg       0.77      0.76      0.76     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = make_pipeline(\n",
    "    CountVectorizer(analyzer=identity, min_df=1, max_df=.92),\n",
    "    TfidfTransformer(use_idf=True),\n",
    "    SGDClassifier(alpha=7e-06),\n",
    ")\n",
    "sgd.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "predicted = sgd.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"wine_variant\"], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hidden-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_f1 = f1_score(test[\"wine_variant\"], predicted_base, average=\"macro\")\n",
    "sgd_f1 = f1_score(test[\"wine_variant\"], predicted, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "vietnamese-narrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7061027413984349, 0.7135274886642256, 0.0074247472657906766)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_f1, sgd_f1, sgd_f1 - base_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "continuous-scott",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025263070847000874"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sgd_f1 - base_f1) / (1 - base_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-constitutional",
   "metadata": {},
   "source": [
    "After optimizing the model for TFIDF Transform and SGD Clasifier. All metrics from the optimized model have the same outputs as the one from the baseline model. With an F1 accuracy score of 0.76, macro average of 0.71,and weighted average of 0.76, the baseline model SGD Classifer or the optimized model seem to be the best two models. However, the opmitized model is slightly better than the baseline model, just a little bit. Therefore, the final model we will use will be TFIDF Transform + SGD Classifier. To be sure, we can compare different model further. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-morris",
   "metadata": {},
   "source": [
    "Use binomial test with a hypothesis of 50/50 chance of getting right or wrong predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "asian-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "convertible-romantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 752, 30926)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare if two classifier give different answers\n",
    "# counting how many right/wrong\n",
    "diff = (predicted == test[\"wine_variant\"]).astype(int) - (\n",
    "    predicted_base == test[\"wine_variant\"]\n",
    ").astype(int)\n",
    "sum(diff == 1), sum(diff == -1), sum(diff == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "challenging-laptop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.229611051836898e-06"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.binom_test([sum(diff == 1), sum(diff == -1)], alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-update",
   "metadata": {},
   "source": [
    "The Binomial test has a really small p-value. This proves that a diviation from 50/50 is very small. There for the test is significant. \n",
    "\n",
    "Use Wilcoxon test to tet the sign of the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "random-painting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=804950.0, pvalue=1.117983801152459e-06)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.wilcoxon(diff, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-albert",
   "metadata": {},
   "source": [
    "P-value is also very small for Wilcoxon test. Therefore, we conclude that the signs are correct. \n",
    "\n",
    "Therefore, the optimized model is clearly better than the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-gambling",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "looking-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "simplified-argument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.70      0.81      0.75      7558\n",
      "        Chardonnay       0.81      0.86      0.84      4861\n",
      "            Merlot       0.79      0.34      0.47      1381\n",
      "        Pinot Noir       0.77      0.87      0.81      9618\n",
      "          Riesling       0.80      0.80      0.80      2421\n",
      "   Sauvignon Blanc       0.84      0.66      0.74      1278\n",
      "             Syrah       0.75      0.54      0.63      3426\n",
      "         Zinfandel       0.85      0.52      0.65      2082\n",
      "\n",
      "          accuracy                           0.76     32625\n",
      "         macro avg       0.79      0.68      0.71     32625\n",
      "      weighted avg       0.77      0.76      0.75     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feed in raw text instead of tokenized text and use tokenizes = tokenize parameter \n",
    "# get the same result \n",
    "sgd = make_pipeline(\n",
    "    CountVectorizer(analyzer=identity, min_df=1, max_df=.92),\n",
    "    TfidfTransformer(use_idf=True),\n",
    "    SGDClassifier(alpha=7e-06),\n",
    ")\n",
    "sgd.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "predicted = sgd.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"wine_variant\"], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "pediatric-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudpickle.dump(sgd, open(\"sgd.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-lotus",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Find the words that the model is using to predict labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "attached-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "alternate-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model \n",
    "sgd = cloudpickle.load(open(\"sgd.model\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-sender",
   "metadata": {},
   "source": [
    "#### Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "informal-pearl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = test[(test['wine_variant']=='Pinot Noir')&(predicted=='Syrah')]\n",
    "len(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "expired-trustee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gorgeous nose of ripe fruit and white pepper. On the palate, this is beautiful and youthful, with great acids and nice tannins which are largely integrate and a nice complement. Wonderful strawberries and great minerality. Wonderful. My #2 WOTN, Group's WOTN.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err['review_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dependent-functionality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Herbs and smoked cheese on the nose, after breathing for a bit it revealed some fruit and roasted meat flavors. Damn, wish i'd gotten to this two years ago. 45 bones.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err['review_text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "essential-childhood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the last of night. meaty dark fruit nose. great weight in the mouth, very precise, just rolled through my mouth and gave taste and taste..great bottle'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err['review_text'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-blame",
   "metadata": {},
   "source": [
    "The common words in this Pinot Noir labels and Syrah are fruity taste and fruity flavor. The algorithm could be very easily confused between the two types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "increased-quantity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err = test[(test['wine_variant']=='Pinot Noir')&(predicted=='Riesling')]\n",
    "len(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "printable-grenada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The nose is clearly oxidized but on the palate the quality shows through - sweet and dense wine. Such a shame the nose was so awful, this wine is a legend!'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err['review_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "usual-precipitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The nose is very mineraly with some dark fruit. It's light on the palate, with hints of citrus and minerals and raspberries. Very dry finish.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err['review_text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "focused-services",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pretty floral this time. Matched up very nicely with Indian cuisine. I keep waiting for this one to close down for a while, but so far it is still all right there.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err['review_text'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-girlfriend",
   "metadata": {},
   "source": [
    "These review texts also has some fruit, floral flavor in them, which could have been easily misclassified as a Pinot Noir. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-quest",
   "metadata": {},
   "source": [
    "#### Model labels and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aware-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sgd.named_steps['sgdclassifier'].classes_\n",
    "scores = sgd.decision_function(test[\"review_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-exploration",
   "metadata": {},
   "source": [
    "View all the labels for the model and the coefficient for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "earned-roommate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cabernet Sauvignon', 'Chardonnay', 'Merlot', 'Pinot Noir',\n",
       "       'Riesling', 'Sauvignon Blanc', 'Syrah', 'Zinfandel'], dtype='<U18')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "destroyed-connecticut",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.03799746, -1.18573332, -1.01243731, -0.18691811, -0.56845076,\n",
       "       -1.21499175, -1.1016332 , -1.40907213])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "modern-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest = scores.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "breeding-blackjack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2574"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "polish-reliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.81886615, -0.94458217, -1.07995904,  0.24736993, -0.70534888,\n",
       "       -1.20539673, -0.97741393, -1.46469992])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[2574]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "structured-paradise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cabernet Sauvignon'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['wine_variant'].iloc[2574]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "facial-theme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow. Drink up. This wine is bursting with fruit after all these years. Fruit dominated strawberry and plum. Great wine.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['review_text'].iloc[2574]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-preparation",
   "metadata": {},
   "source": [
    "This is one example of the review text that would have the highest scores. These kinds of text would be the most sensitive to. As looking at the text, it is quite broard with the fruit flavor.  \n",
    "\n",
    "We can look at the argmin example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "beginning-rebecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6104"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cooked-nightmare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.04366066, -1.06128566, -1.1739224 , -0.93052976, -1.34586669,\n",
       "       -1.30348086, -1.32688622, -1.23352915])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[6104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "handmade-pulse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cabernet Sauvignon'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['wine_variant'].iloc[6104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "falling-video",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LOST MOST OF IT'S OAK AFTER AN HOUR OR SO AND BECAME VERY WELL BALANCED W/ DARK RED FRUIT AND LEATHERY TOBACCO.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['review_text'].iloc[6104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-resistance",
   "metadata": {},
   "source": [
    "### 3. Improve F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "collaborative-flashing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.03799746, -1.18573332, -1.01243731, -0.18691811, -0.56845076,\n",
       "        -1.21499175, -1.1016332 , -1.40907213],\n",
       "       [-1.14327201, -1.39103226, -1.10711436, -0.04917735, -0.51314788,\n",
       "        -1.17378408, -1.10273248, -1.28258014],\n",
       "       [-1.31091204, -1.25753694, -1.14355182, -0.03357089, -0.54624536,\n",
       "        -1.19002576, -1.12713196, -1.46726075],\n",
       "       [-1.38686013, -1.10907669, -1.17670792, -0.08945508, -0.77231138,\n",
       "        -1.18292441, -0.98313824, -1.4566137 ],\n",
       "       [-1.16743885, -1.24821853, -1.05984842, -0.06359701, -0.65494692,\n",
       "        -1.04265347, -1.09804623, -1.40497275]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "other-christianity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.40907213, -1.21499175, -1.18573332, -1.1016332 , -1.03799746,\n",
       "        -1.01243731, -0.56845076, -0.18691811],\n",
       "       [-1.39103226, -1.28258014, -1.17378408, -1.14327201, -1.10711436,\n",
       "        -1.10273248, -0.51314788, -0.04917735],\n",
       "       [-1.46726075, -1.31091204, -1.25753694, -1.19002576, -1.14355182,\n",
       "        -1.12713196, -0.54624536, -0.03357089],\n",
       "       [-1.4566137 , -1.38686013, -1.18292441, -1.17670792, -1.10907669,\n",
       "        -0.98313824, -0.77231138, -0.08945508],\n",
       "       [-1.40497275, -1.24821853, -1.16743885, -1.09804623, -1.05984842,\n",
       "        -1.04265347, -0.65494692, -0.06359701]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sort(axis=1)\n",
    "scores[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "chubby-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = scores[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "differential-switch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0510775657856943, -1.5958055170577996)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margin.max(), margin.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "compound-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       1.00      1.00      1.00         1\n",
      "        Chardonnay       1.00      1.00      1.00         1\n",
      "            Merlot       1.00      1.00      1.00         1\n",
      "          Riesling       1.00      1.00      1.00         1\n",
      "\n",
      "          accuracy                           1.00         4\n",
      "         macro avg       1.00      1.00      1.00         4\n",
      "      weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test[\"wine_variant\"][margin > -1.07], predicted[margin > -1.07]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "purple-study",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       1.00      1.00      1.00         1\n",
      "        Chardonnay       1.00      1.00      1.00         2\n",
      "            Merlot       1.00      1.00      1.00         1\n",
      "          Riesling       0.50      1.00      0.67         1\n",
      "         Zinfandel       0.00      0.00      0.00         1\n",
      "\n",
      "          accuracy                           0.83         6\n",
      "         macro avg       0.70      0.80      0.73         6\n",
      "      weighted avg       0.75      0.83      0.78         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test[\"wine_variant\"][margin > -1.075], predicted[margin > -1.075]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-publisher",
   "metadata": {},
   "source": [
    "To get an F1 accuracy score of around .83, we have to use the margin of -1.075. Or if we increases the margin number to at least -1.075, then the accuracy score would be 1 as well as the macro average and weighted average. However, a model with perfect score would exclude a lot of test sample. Therefore, we still stick with .83 accuracy score. \n",
    "\n",
    "--- \n",
    "### 4. Assign New Labels \n",
    "\n",
    "Another way to improve accuracy is to change the labels. Use a confusion matrix to examine the patterns errors and propose a new labeling scheme. For example, if the model consistently labels “merlot” as “riesling” and vice versa, you might want to create a new label “merlot/riesling”. Is it possible to get better than 0.85 F1 using your classifier trained on a different set of labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-horror",
   "metadata": {},
   "source": [
    "#### Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "heavy-letters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f68be436460>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAIUCAYAAADrDn1xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAACAoElEQVR4nOzdd3gc1fn28e+zau5VslxwoxmwARvbYFOM6SWUFBIgBELoNZBAEhIIECC8/BIIvYROIPSQQKgOBtMNphgwxb3bsiX3pv68f8zIXsmyCtZqttyf69rLu7NnZu+VR9qzz5w5Y+6OiIiISKqJRR1ARERE5LtQJ0ZERERSkjoxIiIikpLUiREREZGUpE6MiIiIpKTsqAOIiIhI6zjsgPa+bHlVi2/3ky/KXnP3w1t8w41QJ0ZERCRDLFtexUev9Wvx7Wb1mp7f4httAnViREREMoQD1VRHHaPFaEyMiIiIpCRVYkRERDKGU+WqxIiIiIhESpUYERGRDBGMiUmfayaqEiMiIiIpSZUYERGRDJJOZyepEyMiIpIhHKfKdThJREREJFKqxIiIiGQQDewVERERiZgqMSIiIhnCgao0qsSoEyMiIpJBdDhJREREJGKqxIiIiGQIB51iLSIiIhI1VWJEREQySPrM16tOjIiISMZwPK3OTtLhJBEREUlJqsSIiIhkCoeq9CnEqBIjIiIiqUmVGBERkQzhpNfAXlViREREJCWpEiMiIpIxjCos6hAtRp0YERGRDOFAtQb2ioiIiERLlRgREZEMkk6Hk1SJERERkZSkSoyIiEiGcNKrEqNOjIiISAap9vTpxOhwkoiIiKQkVWJEREQyRLodTlIlRkRERFKSKjEiIiIZwjGq0qh+oU6MiIhIBtHAXhEREZGIqRIjIiKSIdJtYK86MQJAblZbb5vdOeoYzeLl5VFHSHs9hpRGHaHZiqd3ijpCs3mZ9mWprZR1lHtZ+vQ2EkSdGAGgbXZn9t7mZ1HHaJbK2XOjjpD2zv/PtKgjNNvfDzs06gjNVjlrTtQRJMl86OMTtGWjytNnJEn6vBMRERHJKKrEiIiIZAgHqtOofqFOjIiISAZJp4G96dMdExERkYyiSoyIiEiGcNfAXhEREZHIqRIjIiKSQarTaEyMOjEiIiIZIpixN30OwqTPOxEREZGMokqMiIhIxtDAXhEREZHIqRIjIiKSITRjr4iIiKSsKk+fs5PSpzsmIiIiGUWVGBERkQzhmE6xFhEREYmaKjHSZMP3WspZF39JLOaM+29/nnlshzotnLMvnsKI0UsoK83i5j8PY+a0LgB8//iZHHr0PNxh7sxO3Hz9UCrKs9h2h1Wc/5svyM2toqrKuOvG3Zj2TddWf28AI8au5pxrF5EVc155ohtP31EYSY7mSLbMc99ux7vX9aC6Cnb5ySqGn72i1vNla2K8fklP1izOoboShp2+gp2PW82axdmM/01P1hdnQQwGH7+K3U9dmbCcw/dawlkXfUksBuNe7Mczj+1Yp4Vz9kVfMmL00mBfvn7TvnzMj2dy2NFzMYPXXujP889sB8Bp533FnvsUUVkRY/Gidtxy/R6sW5uTsPfQkGTbLxqTankhNTPXqNYp1o0zs55m9qSZzTSzr83sZTOr+5civv0AM5uSqDwNvOZPt/BczMxuM7MpZvalmU0ys4Et/PrnmNkpLbnNRInFnHMv+YKrLhnFuScdyJiDF9J3wJpabUaMXkrvbdZx5vEHcftfduf8S78AoHv+Bo4+bjYXnzaG808+gFjM2f/ghQD84ryvefzBHbnw1LE8dv9O/OK8r1v9vUHw/s6/fiFXnDSQM8cO4oBjV9Jvh9JIsjRVsmWuroK3r+7BUfcv5KevzGH6i51YPj23VpsvH+tC1+3LOeG/c/nBYwt474YCqsohluXs8/tifvraXI57Zh5f/rPLZuu2lFjMOffXX3DVpaM592c1+/LqWm1GjFpK777rOPOEg7j9r7tz/qWfA9B/4GoOO3ouvz5zDBecOpY99ymi9zZrAfhsUgHnnXIAF5x6AIvmd+AnJ09LSP7GJNt+0ZhUywupmTkZmNnhZjbVzGaY2WX1PN/ZzP5rZp+b2Vdm9ovGtpmQToyZGfBvYIK7b+fuuwB/ABLWVTWz71JVGgDU24kBjgd6A7u5+67AD4CV3yncFrj7Pe7+j5bcZqLsuPMKFi1oT9Gi9lRWxnh7fB9G7VdUq82ofYt449VtAGPqV91o37GCrt2DX+ysrGpy86qIZVWT16aKZSVtAHCHdu0rAWjfvoLl4fLWNmjYehbNyaVoXh6VFTEmPN+F0YetiiRLUyVb5qVftKFz/wo696sgKxd2+N5qZo9vX6uNmVOxLoY7VKw38jpXEcuG9j2qKBhcBkBuB6frduWsW5KYQvFm+/LrfRi1b519eb/FvPFqXzbuyx2CfbnvgDVM/aorZWXZVFfF+PKzfEaPWQzAZ5N6UF0V/En99quudC+I5kMt2faLxqRaXkjNzDVqLjvQ0rfGmFkWcCdwBLALcKKZ7VKn2fnA1+6+OzAWuMnMGvw2k6hKzAFAhbvfU7PA3Se7+ztm1sHMxpvZp2GF49i49bLN7BEz+8LMnjWzdgBmNtzM3jKzT8zsNTPrFS6fYGbXm9lbwEXh4/8zs4/MbJqZ7Re2yzKzv4bVlC/M7Ozw9W4A9jOzyWb2qzrvoRew2N2rw/wL3H1FuL27zezjsKf4p5oVzGyOmeWH90eEeWLh8i5x7WaYWaGZXW1ml4bLRobZPgizTgmXn2pmz5nZq2Y23cz+EredE8Of4RQz+7+45WvN7M9hb3aimW1157F7QSklS9tufFyytA3dCzZs1qa4Vpu2dC8oZVlJW557Ynsefu5/PPb8ONaty+azj3oAcN+tQzjtvK95+LlxnHbB1zx8z85bG/U76d6zguJFm35XShbnkN+rIpIsTZVsmdcWZdOhV+XGxx16VrJuSe3DKbv+bCUrZuby8D7b8sRRA9jvimKszl+h1QuyKfk6j8LdE9MJ2GxfLm67WYeje349+3L+BubO6sSQocvo2KmcvLxKRoxeQkGP2r8HAId8bx6fTOyRkPyNSbb9ojGplhdSM3MNx6jylr81wZ7ADHef5e7lwJPAsXXaONAxLIR0AJYDlTQgUZ2YIcAnW3iuFPiBu+9B0Nm5KQwMMAi41913A1YD55lZDnA7cJy7DwceBP4ct70u7r6/u98UPs529z2Bi4GrwmWnA6vcfSQwEjgzPDR0GfCOuw9195vr5HwaODrs4NxkZsPinrvc3UcAuwH7m9luW/pBhJ2g5wkqOZjZXsAcd19Sp+lDwDnuPhqoqvPcUILK0K7A8WbW18x6A/8HHBg+P9LMvh+2bw9MDHuzbwNn1pfNzM4KO2Mfl1ev39JbCNvW9+asThuvpw106FjOqP2KOO3HB3PysYfSpk0VBxw6H4AjfzCH+24fzKk/PJT7bhvMxb+f3GCORKnv/Xk9byeZpETmOvvEvHfak79zGae+N4vjX5jL29f0oHzNpj9D5euMVy/ozb6XF5PbsToxkbawn9ZuU++azJ/bkWcf24Hrbn6fa26ayOwZnamqqt34+FOmUlVlvDlumxbL3BwpsV/ESbW8kJqZW0F+zedJeDurzvN9gPlxjxeEy+LdAewMLAK+BC6qKSRsSRSjewy43sy+AF4neBM1lYL57v5eeP8xYF+Cjs0Q4H9mNhm4Aoj/6/BUne0/F/77CcHhIoBDgVPC9T8EugN1R6XW4u4Lwtf+PVANjDezg8Knf2JmnwKfAYMJSmMNeYqgEwJwQt3MYZWmo7u/Hy56vM764919lbuXAl8D/Qk6YxPcvdjdK4F/AmPC9uXAi+H9+J9D3fd4r7uPcPcRubF2Db6BkqVtyI/7xpnfo3TjIaFNbdrW+laa32MDy0raMHRECUsWtWP1yjyqqmK8/1Yvdt41GPB50BHzeX9CLwDefaM3O+6yssEciVKyOIeC3uUbH+f3qmBZUTSDMpsq2TJ36FnJ2sWbDgGtLcqmfY/aX6K+/Vcntj10LWbQpX8FnbapYMWs4BttVQW8ekFvdjxmNdsdtjZhOUuWtq29Lxds2HxfLm5T774MMO6l/lx0+lh+d8G+rFmdw6IFHTa2O+jweYzcewk3/mk4wZ+61pds+0VjUi0vpGbmeNXEWvwGlNR8noS3e+u8bL1fhes8PgyYTDCUYyhwh5l1aui9JKoT8xUwfAvPnQQUAMPdfSiwBKj5C1L3DTnBG/8qrJYMdfdd3f3QuDbr6qxTFv5bxaazrwy4MG4bA919XGNvwt3L3P0Vd/8NcD3w/bCCcylwUFgxeikufyWbfqbxfxU/ALY3swLg+2zqaNVo7K9dWdz9mvfV0DoV7hu/F8T/HL6zad92oc826yjstY7s7GrGHLSQD9+tfZTqw3d7cuDhCwBn0ODlrFubw4plbShe0pZBQ1aQl1cJOLuPKGb+3OAP//KSNuw6bBkAuw8vYdH89kRh6uR29BlYTmHfMrJzqhl77EomjuscSZamSrbMPXYtZdWcHFbPz6aqHKa/1IkBB9X+9ezQu5IFHwQd5vUlWaycnUunvuW4w5t/6EnX7coZetrKhOac9m0X+vSN25cPXsiH7/Ws1SbYl+dTd18G6Nwl+HUsKFzP3vsv5q3Xgy+Tw/dawnEnTeeay/airCy6Ez+Tbb9oTKrlhdTMnAQWAH3jHm9DUHGJ9wvgOQ/MAGYDOzW00UT9pr1BUG05093vg2DMB9AO6AwsdfcKMzuAoKpQo5+ZjXb3D4ATgXeBqUBBzfLw8NKO7v5VM/K8BpxrZm+Er7sjsBBYA3SsbwUz2wMocvdFZhYjOHT0BdCJoOO0KhxrcgQwIVxtDkHn7RXgRzXbcnc3s38DfwO+cfdl8a/l7ivMbI2ZjXL3iQTVmsZ8CNwajsFZQfDzur0J630n1VUx7r55V67920RiWc7/XuzHvNmdOOL7cwB45T8DmPRBD0aMXsL9T4/feFoqwNSvu/Lem7249aG3qaoyZk3rzCvPB//tt/3f7px90RRiWU5FeYzb/7J7ot5CI+/PuPPyPlz/+CxiWTDuyW7MnRbNIOOmSrbMsWzY76piXjhtG7wKdj5uNd13KGfK48Ef9yE/XcXI85cx/nc9eeJ7/cFh9G+KadutmkUft2HqfzrRfVAZTx7dD4BRlyxjwNi631G2XnVVjLv/thvX/u0DYjHnfy+F+/KxswF45fmBTPqgMNiXn3q91r4M8Ic/f0SnTuVUhttZuyaoJJ3zqy/JyanizzcHBdVvv+rGnTe2/v6cbPtFY1ItL6Rm5hruRHUV60nADmEhYCHB51zdE2vmAQcB74Sfr4OAWQ1t1DxBB/LCMRu3EHyolxJ8wF9M8IH7XyCHoGy0D0FHAOBlgjEcewPTgZPdfb2ZDQVuI+gAZQO3uPt9ZjYBuNTdPw5fc+Pj8MP9Y3cfEHZCrgOOJqhgFBNURNYDrwL5wMPx42LM7HCCsTd54aKPgPPcvdTMHgb2IvjhlgEvuPvD4UDiBwiqSx8CI9x9bLi9EQT/iae6+yPhsquBte5+YzhW5j6CDtIEYIy772Nmp4bbuSBc50XgRnefYMHp4b8P39PL7v7bsM1ad+8Q3j8OOMrdT23o/6tzXk/fe5ufNdQk6VTOnht1hLR3/vRoThPeGn8/7NDGGyWZyllzoo4gSeZDH89qX97ixyQHDOnoVzw3tKU3y5mD3v0kHCu6RWZ2JEG/IAt40N3/bGbnQHC2bthveJjgxBoDbnD3xxrcZqI6MdI8ZtbB3deG9y8Dern7Ra31+urESH3UiWkd6sRIXenYiUkEzdibPL5nZr8n+D+ZC5wabRwREUk3TmSHkxJCnZgk4e5PsfmZViIiIrIF6sSIiIhkkHS6irU6MSIiIhnCMaqbNsNuSkif7piIiIhkFFViREREMkg6HU5Kn3ciIiIiGUWVGBERkQzhQHUanWKdPu9EREREMooqMSIiIhnDqIroCuuJoE6MiIhIhtDhJBEREZEkoEqMiIhIBkmnw0mqxIiIiEhKUiVGREQkQ7hbWo2JUSdGREQkg1SpEyNpp7KS6qUlUaeQJNMja03UEZrN2+ZFHUFEWok6MSIiIhnCgWoN7BURERGJlioxIiIiGcM0JkZERERSTzBjrw4niYiIiERKlRgREZEMUpVG9Yv0eSciIiKSUVSJERERyRCOaUyMiIiISNRUiREREckg1WlUv1AnRkREJEO4Q5UOJ4mIiIhES5UYERGRDKKBvSIiIiIRUyVGREQkQwSnWKdP/UKdGBERkQxShQ4niYiIiERKlRgREZEMkW5XsVYnRpps+H4rOOeK2cSy4NWne/DMvdvUaeGc88fZjNx/JWUbYtz0u+2Z+XUHcnKr+evjU8jJrSYr23n31e48dls/AE7/3Rz2OmAFlRXG4nlt+Ntl27NuTTS75Yixqznn2kVkxZxXnujG03cURpKjOZIt8/S3OvHKNX3xatjjJyXsd+6SWs+Xro7xr18PZNWiXKqrjH3OWMKwHy8D4Ob9hpDbvppYlhPLcs5+4dtWyTx8xGLOPm8ysZjz2isDeeapnWs9v03f1fzq0klsv/0KHnloCM89uxMA+QXrueS3H9K1Wylebbz68rY8/+8dWyVzY5Jtv2hMquWF1MycjjK+E2NmPYFbgJFAGTAH+A9wjLsf1UKvMQB40d2HtMT2ohCLOedfPYs/nDqYkqJcbv3XF3z4RjfmzWi3sc3I/VfSu38ppx88jJ2GruWCa2bxq+N2o6LcuOyUwZSuzyIru5obn5zCx2935dvJHfnsvS48dGN/qquM034zh+PPWcCDfx0Qzfu7fiG/P2FbShbncPvL05n4WmfmTW/T6lmaKtkyV1fBS1f145R/TKNTzwru/f5ODDp4FT12KN3Y5qNHe1CwfSkn3T+Tdcuyuf3gwex67HKycx2AUx+fSvtuVa2WORar5rwLP+Xy3+1PSUlbbrnjdSZ+0Jv58zpvbLNmTS733DmM0fssrLVuVZVx/9+HMnNGV9q2reC2u/7Hp58U1lo3Csm2XzQm1fJCambeJL0G9qbPO/kOzMyAfwMT3H07d98F+AOwVV1qM0u7zuGOu61l0dy2FM1vQ2VFjLdeymfUQctrtRl18HLG/6cAML6d3JEOHSvpWlAOGKXrswDIznaysx0PPrP49N0uVFcFpc1vJ3ckv2d5K76rTQYNW8+iObkUzcujsiLGhOe7MPqwVZFkaapky7zw8/Z0619Kt37lZOc6Q45awbf/61K7kUH5uhjuUL4+RtsulcSyPZK8ADsOWs6iRR0oKupAZWUWb0/ox+i9F9Vqs2plG6ZP60ZVZe0S/IrlbZk5oysAGzbkMG9eJ/LzN7Ra9i1Jtv2iMamWF1Izc7xqrMVvUcnoTgxwAFDh7vfULHD3ycA7QAcze9bMvjWzf4YdHszsSjObZGZTzOzeuOUTzOx6M3sLuMjMhpvZ52b2AXB+zfbNrI2ZPWRmX5rZZ2Z2QLj8VDN7zsxeNbPpZvaXuHXWmtmfw+1NNLPCcPnRZvZhuJ3XzazQzGLh+gVhm5iZzTCz/K35QeX3LKN4ce7GxyVFuXQvrN3h6F5YTsnivLg2eeSHbWIx544XJvPExEl89l5npn7ecbPXOPS4pUx6q+vWxPzOuvesoHhR3PtbnEN+r4pIsjRVsmVeXZRD57jX79yrnDVLcmq12euUpRTPbMuNo3bjriN24Yg/zidW81fI4NGf78g9x+zEx09s1e7aZN3zN1BSvKmaWFLSlu7foSPSo3Ad222/km+/7d6S8b6TZNsvGpNqeSE1M6erTO/EDAE+2cJzw4CLgV2AbYF9wuV3uPvI8NBQWyD+kFMXd9/f3W8CHgJ+6e6j62z3fAB33xU4EXjEzGpqkEOB44FdgePNrG+4vD0w0d13B94GzgyXvwuMcvdhwJPAb929GngMOClsczDwubuX1H2DZnaWmX1sZh+Xe2ndpxtX5wu02ebfqGsqLtXVxgXHDOXk/Uaw425r6b/DulrtTjh3AVWVxpsvtM6HV11WzxcJj65A0CQpkblOxhlvd6Lnzuu5dOIXnPPiN7x0dT9K1wR/hk5/Zirn/PcbfvbgDD56tIA5H3VIfLwW+Bm2aVPB5Ve+z713D2XD+pzGV0iwlNgv4qRaXkjNzDVqrp3U0reoZHonpiEfufuCsFMwGRgQLj8grH58CRwIDI5b5ykAM+tM0KF5K1z+aFybfWseu/u3wFygZjTgeHdf5e6lwNdA/3B5OfBieP+TuCzbAK+FWX4Tl+VB4JTw/mkEHarNuPu97j7C3UfkWsPHckuK8ijotanykt+znGVLczdrk9+rLK5N2WZt1q3J5osPOzNizMqNyw7+wVL2PGA5f7lkBzb71GslJYtzKOgd9/56VbCsKPoPpIYkW+ZOPStYtXjT669anEvHHrW/nX72bD67HLYSM+g+oIyufcsomRXse50Kg7Yd8ivZ+dCVLPy8fcIzlxS3Jb9g/cbH+fkbWL6sbZPXz8qq5vKr3mfCG/14/926A92jkWz7RWNSLS+kZuZ0lemdmK+A4Vt4rizufhWQHVZM7gKOCysp9wHxn/415QVjszrFRg19Sm/2muH9CveN/fz45bcTVIZ2Bc6uyeLu84ElZnYgsBfwSgOv2STTvuxA7wEbKNymlOycavb/XgkTx3er1Wbi+K4c9P1iwNlp6BrWrclmRXEunbtV0L5jJQC5eVUM23sl82cFHxTD91vBj89ayJ/O2Zmy0qytjfmdTZ3cjj4DyynsW0Z2TjVjj13JxHHRDtBsTLJl7r3bOpbPacOK+blUlhtTXuzKTgevrNWmc+9yZr0fHEpcW5xNyaw2dO1bRvn6GGVrgz9H5etjzHy3Ez12TPz4kmlTu9G7z1oKe64lO7uKMWPnMfGD3k1c27n4kknMn9eJf/9rUEJzNkey7ReNSbW8kJqZ41V7rMVvUUm7AajN9AZwvZmd6e73AZjZSGD/LbSv6bCUmFkH4Djg2bqN3H2lma0ys33d/V02HdqB4HDQScAbZrYj0A+YCuzxHfJ3BmpOmfh5nefuJzis9Ki7b/XpHtVVxt1/2pbrHvyarCxn3LOFzJvRjiNPLALg5Sd6MmlCV0buv5IHx39K6YYsbr5sewC6FpRz6V9mEIs5FnPeeSWfj94MOkDnXTWbnNxq/vzwV0AwuPeOK7fb2rjf6f3deXkfrn98FrEsGPdkN+ZOS+4zDZItc1Y2HHn1PB79+Q5UVxvDflxCjx1LmfTP4BDhyJNK2P/CxfznNwO48/BdADjkdwtp362K5fNyefKc7Ta+r12PWc4O+69OeObq6hh337EH1/2/t4nFnHGvDWTe3M4cedQMAF5+cXu6dt3ArXe+Trt2FVS78f0fTufsMw5n4MCVHHTIXGbP6szt94wD4JEHd+Xjj3olPHdDkm2/aEyq5YXUzJyuzFPlQF6CmFlvglOshwOlbDrF+tiaU6zN7A7gY3d/2MyuA04I280H5rr71WY2AbjU3T8O1xlOcFhnPfAaQfVmSFjNuSd8vUrg1+7+ppmdCoxw9wvC9V8EbnT3CWa21t07hMuPA45y91PN7FjgZoKOzERgpLuPDdvlAMuAPcPDVg3qnJXvo9q1yBnlraZ63brGG8lW+dOsLQ0ZS17XHP3TqCM0W9VXU6OOIEnmQx/Pal/e4sfXu+9c4Ec+fGxLb5bHRj3wibuPaPENNyLTKzG4+yLgJ/U8dV9cmwvi7l8BXFHPdsbWefwJsHvcoqvD5aXAqfWs/zDwcNzjo+Lud4i7/yxh9cfdnweer+99ha/9eVM6MCIikjmiPCW6pWV8JyYdmdllwLnUPowlIiKSVtSJSUPufgNwQ9Q5REQkuaTbtZMy/ewkERERSVGqxIiIiGSQdLp2kjoxIiIimcJNh5NEREREoqZKjIiISIZw0usUa1ViREREJCWpEiMiIpJB0mlMjDoxIiIiGULzxIiIiIgkAVViREREMogqMSIiIiIRUyVGREQkQzia7E5EREQkcqrESMAMy82NOkXzrFsXdYK0N6pNVtQRms1nzo06gkhSS6fJ7tSJERERyRSugb0iIiIikVMlRkREJENosjsRERGRJKBKjIiISAZJp0qMOjEiIiIZQvPEiIiIiCQBVWJEREQyiKsSIyIiIhItVWJEREQyiGbsFRERkZTjmrFXREREJHqqxIiIiGQQDewVERERiZgqMSIiIhkjvSa7UydGREQkg+hwkoiIiEjEVIkRERHJEI5OsRYRERGJnCox0mTD913G2ZfNIJblvPavXjxzf/86LZyzfz+DkWOWUbYhi79dvhMzv+m48dlYzLn16U9YtiSXq8/fbePyo3+6gKN/upCqKmPS29158KbtWukd1TZi7GrOuXYRWTHnlSe68fQdhZHkaI5kyzzpzY7c88c+VFUbR5y4jOMvXFrr+TUrs/jbr/uyeG4eOXnVXPK3+QzYqZTyUuOSH25PRXmMqkrY73urOOU3RQnLOXzMSs65ci6xmPPq0z145p7edVo451w5l5FjV1JWGuOm32zHzK/ak9+rjEtvnEnXggq82njlyR48/3DPWmv+6IzFnPGHeRw/fA9Wr8hJ2HtoSLLtF41JtbyQmpkB8GDCu3ShSkwrMzM3s0fjHmebWbGZvdjM7QwwsymNtBlqZkd+16zxYjHnvMunc+U5u3HOMXuy/5FL6bvdulptRuy3nD79N3DGEXtx29U7csGV02o9f+zJC5g/q12tZbvtuYJRB5Zw3g9Gcu6xe/Kvh/q2RNxmi8Wc869fyBUnDeTMsYM44NiV9NuhNJIsTZVsmauq4M4/bMN1/5zFfRO+5c3nuzJ3Wl6tNk/eVsh2gzdwz/ip/ObWedx9ZR8AcvKcvzwzk3ten8rd/5vKxxM68s0n7ep7ma0Wiznn/2kOf/zFIM4+bDfGHr2Mftuvr9Vm5NhV9B5QyukH7s5tfxjIBdfODt5jpXHf9f05+9Dd+dWPBnPUyUtqrZvfq4xh+65iycLchGRvimTbLxqTankhNTOnK3ViWt86YIiZtQ0fHwIsbM4GzKypFbShQIt0YnbcdTWL5relaEFbKitivP1yD0YfUFKrzagDSxj/QiFgTP2iM+07VtI1vwyA7oWljByzjNf+1avWOt87fhHP3N+PyopgV1y1PJo//oOGrWfRnFyK5uVRWRFjwvNdGH3YqkiyNFWyZZ76WTt6DyijV/9ycnKdsceu4IPXOtdqM296HkP3XQtAvx3KWDI/lxXF2ZhB2/bVAFRWGFUVhiXosP2Ou69l0dw2FM1vQ2VFjLde7MaoQ1bUajPq4BWM/3c+YHw7uSMdOlXRtaCcFcW5zPyqPQAb1mUxf0Ybuves2Lje2VfM5YEb+gYDDyKSbPtFY1ItL6Rm5njVWIvfoqJOTDReAb4X3j8ReKLmCTNrb2YPmtkkM/vMzI4Nl59qZs+Y2X+BcfEbM7M2ZvaQmX0ZrnOAmeUC1wDHm9lkMzt+awJ3LyyjZPGmb9UlS/LoXlhWq01+jzKKi2q3yQ/bnH3ZDB68aTuqq2tvt/eA9Qwevoqbn/iE/3v4M3YYsnprYn5n3XtWULxoUweqZHEO+b0qGlgjesmWeVlRDgW9N71+fq8KShbXPpwycJdS3nsl6Nh8+1k7lizI3dimqgrOPXgQx+82hGFj1rDTHrWrIy0lv2c5xYvjf265dC+s/XPr3rO89v5elEt+z/JabXr0KWO7weuZOjno1Ox10ApKinKZ/W37hORuqmTbLxqTankhNTPXcIJTrFv6FhV1YqLxJHCCmbUBdgM+jHvucuANdx8JHAD81cxq/iqOBn7u7gfW2d75AO6+K0Gn6BGC/9srgafcfai7P1U3hJmdZWYfm9nH5d5wKbS+XXSzHbeeRu7GnvuXsHJ5LjO+7rjZ81lZTodOlfzqxD144Kbt+P1NXxPF19j6vvUn+3HjZMtc32vXzXj8BUtYszKLcw8exAsP5rP9kA3EsoIVs7Lg7ten8s9Pvmbq5HbM+bZNK6QO1cle/89208I27aq44q5p/P3a/qxfm01emypOOH8hj96yTYKDNi7Z9ovGpFpeSM3M6UoDeyPg7l+Y2QCCDsfLdZ4+FDjGzC4NH7cB+oX3/+fuy+vZ5L7A7eG2vzWzucCOTchxL3AvQOfsggZ/BUuW5JHfa1PlJb+wjOVLczdrU9CzdptlS3PZ99BiRo0tYeR+y8jJq6Zd+youveFrbrxsF0qW5PH+60HZftqXnfBq6NS1gtUrWvewUsniHAp6b/qmnd+rgmVF0QzKbKpky5zfq4LiRZtev2RxTq1DLQDtO1Zz6S3zgeCP/s/32oWe/WpXODp0rmL30WuZ9GZHBuzU8uMMSopyKegV/3MrZ9nS2j+3ksW54f4edLzze5azbEnQJiu7mivums6bL+Tz/mvdAOjVv4ye25Rx10tfbmx/+3+ncPH3B7OiRPtyQ1ItL6Rm5k2im7HXzA4HbgWygPvd/YZ62owFbgFygBJ337+hbaoSE50XgBuJO5QUMuBHYfVkqLv3c/dvwufWUb+E75HTpnSkd78NFPbZQHZONWOOXMrEN/NrtfnwzXwOOmYJ4AzabRXr1mazoiSPh2/ZllMO2ptfHDqa/7t0F774sAs3XrYLABPH57P7XisB6NN/Pdk5HskZHVMnt6PPwHIK+5aRnVPN2GNXMnFc58ZXjFCyZR40dD0LZ+dRNC+XinJjwvNdGXVo7cODa1dlUVEe7K6vPN6NIaPW0r5jNSuXZbF2VRYAZRuMT9/pSN/tyzZ7jZYw7YsO9B5QSuE2pWTnVLP/UcuZ+HrXWm0mju/CQT8oAZydhq5h3ZosVhTnAs7FN8xm/sy2/PuBTeO75kxtx4l7DufUMcM4dcwwSopyufDoIa3egYHk2y8ak2p5ITUzR83MsoA7gSOAXYATzWyXOm26AHcBx7j7YODHjW1XlZjoPAiscvcvw55njdeAC83sQnd3Mxvm7p81sq23gZOAN8xsR4LKzVRgB2q+Sm6l6qoYd/95B6679wtiMWfcv3sxb2Z7jvxJMCb55af7MOntbowcs4wHXvmQstIsbr5iUKPbHffvXlx87bfc9Z+PqKyI8bfLd6IV+mSbqa4y7ry8D9c/PotYFox7shtzp7Xi4YzvINkyZ2XD+X9ewB9+ui3VVcahJyxnwKBSXvxHdwCOOmUZ86bn8deL+hOLOf13LOVXNwVVmeVLcrjxon5UVxvV1TDm6JWMOiQx46Oqq4y7rx7AdY9MJSvmjHumgHnT23HkT5cA8PLjhUx6swsjx67kwTc/p7Q0xs2/3RaAwSPWcvAPS5j9bVvueDGoujxyY18mTeiSkKzfRbLtF41JtbyQmpnjRXToa09ghrvPAjCzJ4Fjga/j2vwUeM7d5wG4+9LNtlKHuQ7ktSozW+vuHeosGwtc6u5HhWct3QLsTfBpPidcfiowwt0vCNcZALzo7kPCsTX3AMOBSuDX7v6mmXUj6BTlAP+vvnExNTpnF/jozj9o0feaaFUrVjTeSLbKa4smRx2h2Y7YdlTUEZqtulSn50ptH/p4VvvyFv9G126H3r79385o6c3y5THXzgXiT1m9NxyyAICZHQcc7u5nhI9PBvaq+UwLl91C8Hk1mOAL+K3u/o+GXleVmFZWtwMTLpsATAjvbwDOrqfNw8DDcY/nAEPC+6XAqfWssxwYufWpRUREGlTi7iMaeL7e80PqPM4m+DJ+ENAW+MDMJrr7tM3WjFtBREREMoB7ZFexXgDEz2a6DbConjYl7r4OWGdmbwO7A1vsxGhgr4iIiCTaJGAHMxsYzmN2AsEJLvGeB/YLZ7JvB+wFfEMDVIkRERHJIFGcYu3ulWZ2AcE4zSzgQXf/yszOCZ+/x92/MbNXgS+AaoLTsBu8vI46MSIiIhkkqvN53P1l6syN5u731Hn8V+CvTd2mDieJiIhISlIlRkREJINEea2jlqZKjIiIiKQkVWJEREQyhBPtVadbmioxIiIikpJUiREREckg6XSxIXViREREMkV0M/YmhA4niYiISEpSJUZERCSTpNHxJFViREREJCWpEiMiIpJB0mlMjDoxAkDHnco54Ol5UcdolteHdIw6Qto79LifRx2h2XLyS6KO0GzVCxZGHUEySFTXTkoEHU4SERGRlKRKjIiISIZw0utwkioxIiIikpJUiREREckUDqRRJUadGBERkQyigb0iIiIiEVMlRkREJJOoEiMiIiISLVViREREMobpFGsRERGRqKkSIyIikknSaEyMOjEiIiKZwjVjr4iIiEjkVIkRERHJJGl0OEmVGBEREUlJqsSIiIhklPQZE6NOjIiISCbR4SQRERGRaKkSI99JybtZTLuhDV4FfX5UwYAzyms9P+fBHIpeygHAq2DdrBj7v7OW8uXGl5e23dhuw4IY211QRr+TK1o1f31GjF3NOdcuIivmvPJEN56+ozDqSI1Ktswjhi7k3F9MIhZzXh2/PU/9Z9daz/ftvYpLzn+P7bddzsNPDOPZFwZvfK59u3J+fe77DOi3Enfjprv25ptpBQnJOXxUMWdd8jWxmDPu+b4884/t6rRwzr7ka0bsXUxZaRY3X7MbM6d2pk+/tVx2/WcbW/XsvYHH7t2B558cyM/OnsaoMUtwh5XL87j5mt1YXtImIfkbk2z7RWNSLS+kZuaN0qgSo05MM5hZFfAlwc/tG+DnwC7AKe7+y++wvS7AT939ri0878Df3P2S8PGlQAd3v7qBbZ4DrHf3fzQ3T1N5FUy9rg3D7ltPm57OR8e3I/+ASjpsV72xzYDTKhhwWtAxKZ6Qxbx/5JLTGXI6O6P+tX7jdt45sD0FB1UmKmqTxWLO+dcv5PcnbEvJ4hxuf3k6E1/rzLzp0XwINUWyZY7FqrngjA+57JpDKFnejttveJkPPu7LvAVdNrZZszaXux7ck733nL/Z+ued9hGTJvfh2pvGkp1dRV5uVYJyOuf+9iuuuGBPSpa24eZH3mPiOz2YP7vjxjYj9i6md9/1nPmj/Rk0ZCXn/24Kvz5tHxbO68CFP9tv43b+8dJ43p/QE4B/PTaQx/6+IwBH/2QOJ54xnTtv2HXzAAmWbPtFY1ItL6Rm5nSlw0nNs8Hdh7r7EKAcOMfdP/4uHZhQF+C8Bp4vA35oZvlN3aC731NfB8bMWqzDuurLGG37VdOurxPLgcIjKil+Y8ubL3o5h55Hbt5RWT4xi7Z9nba9o/9aMGjYehbNyaVoXh6VFTEmPN+F0YetijpWg5It86Dtl7GoqCNFSztSWZnFW+8NYO+RtTsrK1e3ZdrMfKqqag8sbNe2nF13Xsqr47cHoLIyi3XrcxOSc8fBK1m0oB1Fi9pRWRnj7XG9GDVmSa02o8Ys4Y2X+wDG1Cldad+xkq7dS2u12X1kCYsXtKe4KKgsbliXs/G5Nm0rI5tQLNn2i8akWl5IzcwbOeDW8reIqBPz3b0DbG9mY83sRQAzu9rMHjSzCWY2y8w2dm7M7NdmNiW8XRwuvgHYzswmm9lf63mNSuBe4Fd1nzCz/mY23sy+CP/tF5fh0vD+BDO73szeAi5qqTdetjRGm56bqi5tCqspW1r/Tly1AZa9m02PQzY/XFT0Sg49j4z+MBJA954VFC/a9KFZsjiH/F7JkW1Lki1zfrf1FJe03/i4eFk7undb36R1exauZeXqPC49/33u+ut/+dU579MmLzHvpXtBKSVLNn1jLlnalu4FZbXb9CiluFabNnTvUbsTM+aQxbw1rletZaecO5WH//sGYw9fxGN/3yEB6RuXbPtFY1ItL6Rm5njuLX+Lijox30FY1TiC4NBSXTsBhwF7AleZWY6ZDQd+AewFjALONLNhwGXAzLC685stvNydwElm1rnO8juAf7j7bsA/gdu2sH4Xd9/f3W+q532cZWYfm9nH65aX17du/erbYbfQES+ekE2XYVXk1ElfXQElE7LocWj0h5IArJ78Uf5iNkXSZbbNX7ypebKyqtlh2+W8OG5HzvvN0ZSWZXP8D6a0cMBAfT+3zdrUtzDu22Z2djV7jVnCu+Nrd2L+cfcgTj36QCa82pujfzx364J+R0m3XzQi1fJCamZOV1vsxJjZ7WZ225ZurRkyibQ1s8nAx8A84IF62rzk7mXuXgIsBQqBfYF/u/s6d18LPAfs15QXdPfVwD+AuoesRgOPh/cfDV+jPk81sO173X2Eu49o363ppfu8wmpKizbtOqVLYuQV1P8bvOSV7HqrLSXvZNNx52ry8pPjN79kcQ4FvTd15PJ7VbCsKKeBNaKXbJlLlrWnIH/dxscF3dezfEW7Jq9bvKwd304PBvK+M7E/2w9cnpicS9uQX7ipqpLfYwPLivM2a1NQq01prTYj9i5m5redWbm89no1JrzWh70PLGrh5E2TbPtFY1ItL6Rm5lo8AbeINFSJ+Rj4pIFbJqoZEzPU3S909/rKF/F16SqCQcBbe8DwFuB0oH0Dbba0G63bwvLvrNOQajbMi7FhgVFdEXRUCg7YvKJSuQZWfFz/c0terr9zE5Wpk9vRZ2A5hX3LyM6pZuyxK5k4rm7xK7kkW+apM7rTp9caevZYQ3Z2FfvvM4cPJvVt0rorVraleFl7tukdjCsYtuti5i1IzHuZ9nVn+vRdR2Hv9WRnVzPm0MV8+E7tM0s+fKeQA49cCDiDhqxg3dpsVizbdHhpzKGLNjuU1Lvvpl+1UWOWsGBOh4Tkb0yy7ReNSbW8kJqZ09UWR2O6+yPxj82svbu3+AdihngbeNjMbiDo0PwAOBlYA3RsaEUAd19uZk8TdGQeDBe/D5xAUIU5CXg3AbnrFcuGQX8o5bOz2+FV0PsHFXTYvpoFTwXfRLY5PuicLB2fTfe9K8mq82W8agMs/yCbna8qrbvpyFRXGXde3ofrH59FLAvGPdmNudOS+0yDZMtcXR3jjvv35PorXicWc157Y3vmLujC9w6dCsBL4wbRtcsG7vi/l2jXtgJ3+MH3vuHMi49h/YZc7nxgTy676F2ys6soWtKRG+/cOzE5q2Lc/dfBXHvbR8Ri8L//bsO8WR054ofB4Z9XnuvPpPcKGLH3Uu5/7i3KSmPcfO1uG9fPy6ti2F4l3PH/htTa7qnnf0uf/uvwamNpUVvuvKH2860l2faLxqRaXkjNzLWk0VWszRs5kGdmowkOm3Rw935mtjtwtrs3dFZNWjKzte7eoc6yscCl7n6UmV0NrHX3G8PnpgBHufscM/s1cFq42v3ufkvY5nFgN+CVuuNi4l/PzAqB2cBf3P1qMxtA0KHJB4qBX7j7vPgMZjYhzPZxY+9tmyGd/cKnRzf3RxKp14c02v+TreR77x51hGbLmVcSdYRmq1ywMOoIkmQ+9PGs9uUt3tvIG7CN97yixc7z2Gjemb/9xN1HtPiGG9GU025vIRio+gKAu39uZmMSGSpZ1e3AhMsmABPC+1fXeW5I3P2/AX+rZ/2fNuX13H0J0C7u8RzgwHrWuTru/tgtbVtERDJTPWPwU1aT5g5x9/lWezh2YmahEhERkcSJeCBuS2tKJ2a+me0NuJnlEpwl801iY4mIiIg0rCmdmHOAW4E+wELgNeD8RIYSERGRRIh2ht2W1mgnJpzv5KRWyCIiIiLSZI3O2Gtm25rZf82s2MyWmtnzZrZta4QTERGRFpYhk93VeBx4GugF9AaeAZ5IZCgRERFJkAzrxJi7P+ruleHtMdJqbLOIiIikoi2OiTGzbuHdN83sMuBJgs7L8cBLrZBNREREWloalSEaGtj7CcFbrRnGfHbccw5cm6hQIiIiIo1p6NpJA1sziIiIiCSYk1mnWAOY2RBgF2DjFa7c/R+JCiUiIiKJkVGXHTCzq4CxBJ2Yl4EjCK6YrE6MiIiIRKYpZycdBxwEFLn7L4DdgbyEphIREZHEyLBTrDe4ezVQaWadgKWAJrsTERGRSDVlTMzHZtYFuI/gjKW1wEeJDCUiIiLSmKZcO+m88O49ZvYq0Mndv0hsLBEREZGGNTTZ3R4NPefunyYmkkRhzdfZvDGiIOoYzVQadYC0l70q9X7G17zz76gjNNsfBu4ZdQTJIJlydtJNDTznwIEtnEVEREQSLRPmiXH3A1oziIiIiEhzNGmyOxEREUkDEZ8S3dKacoq1iIiISNJRJUZERCSTZFIlxgI/M7Mrw8f9zExD6UVERFKQecvfotKUw0l3AaOBE8PHa4A7E5ZIREREpAmacjhpL3ffw8w+A3D3FWaWm+BcIiIikgiZdDgJqDCzLMK3bWYFQHVCU4mIiIg0oimdmNuAfwM9zOzPwLvA9QlNJSIiIomRRlexbsq1k/5pZp8ABwEGfN/dv0l4MhEREWlRUQ/EbWmNdmLMrB+wHvhv/DJ3n5fIYCIiIiINacrA3pcIikUGtAEGAlOBwQnMJSIiIomQCddOquHuu8Y/Dq9ufXbCEomIiIg0QbNn7HX3T81sZCLCiIiISIJl2JiYX8c9jAF7AMUJSyQiIiLSBE2pxHSMu19JMEbmX4mJIyIiIomUMWcnhZPcdXD337RSHhEREUmkTOjEmFm2u1eGA3lFGD5mJedcOZdYzHn16R48c0/vOi2cc66cy8ixKykrjXHTb7Zj5lftye9VxqU3zqRrQQVebbzyZA+ef7hnrTV/dMZizvjDPI4fvgerV+S03puKM2Lsas65dhFZMeeVJ7rx9B2FkeRojmTLPHzEYs4+bzKxmPPaKwN55qmdaz2/Td/V/OrSSWy//QoeeWgIzz27EwD5Beu55Lcf0rVbKV5tvPrytjz/7x1bJfO0tzrz4p/6UV1tjDy+mP3PXVzr+dLVWTz9q21ZuSiP6irY78wihv+4BIANq7N47ncDWTKtLWbwo7/Mpt8ea1sld0OSbb9oTKrlhdTMnI4aqsR8RDD+ZbKZvQA8A6yredLdn0twtpRkZlXAlwQ/29nAye6+0sx6A7e5+3HfYZtzgBHuXmJm77v73i0augliMef8P83hD6fsRElRLrf+5ys+fL0L82a029hm5NhV9B5QyukH7s5OQ9dywbWz+dUPh1BVadx3fX9mftWetu2ruO2FKXz2bqeN6+b3KmPYvqtYsjC6S3LFYs751y/k9ydsS8niHG5/eToTX+vMvOltIsvUmGTLHItVc96Fn3L57/anpKQtt9zxOhM/6M38eZ03tlmzJpd77hzG6H0W1lq3qsq4/+9DmTmjK23bVnDbXf/j008Ka62bCNVV8MKV/Tnt0al06lnOXccOZqeDV1C4Q+nGNhMf7UGPHTZwygPTWbssm5sP2o3dj11Gdq7z4p/6s+P+qzjp7hlUlhsVpU2ZBD2xkm2/aEyq5YXUzLxRmk1215TfuG7AMuBA4Cjg6PBfqd8Gdx/q7kOA5cD5AO6+6Lt0YOqKogMDsOPua1k0tw1F89tQWRHjrRe7MeqQFbXajDp4BeP/nQ8Y307uSIdOVXQtKGdFcS4zv2oPwIZ1Wcyf0YbuPSs2rnf2FXN54Ia+kZY4Bw1bz6I5uRTNy6OyIsaE57sw+rBV0QVqgmTLvOOg5Sxa1IGiog5UVmbx9oR+jN57Ua02q1a2Yfq0blRV1p6nYsXytsyc0RWADRtymDevE/n5GxKeecHnHejev4xu/crIznV2O3oZ3/yva+1GBmXrsnCH8vUx2napJJbtlK6JMeejjow4PjjPITvXadupKuGZG5Ns+0VjUi0vpGbmdNVQJ6ZHeGbSFILKwhTgq/DfKa2QLR18APQBMLMBZjYlvJ9lZn81s0lm9oWZnR0u72Vmb5vZZDObYmb71d2gma0N/x1rZhPM7Fkz+9bM/mlmFj53ZLjsXTO7zcxe3No3kt+znOLFmyolJYtz6V5YUatN957llCzO29SmKJf8nuW12vToU8Z2g9czdXLQqdnroBWUFOUy+9v2Wxtxq3TvWUHxovj3l0N+r4oG1ohesmXunr+BkuJNlbmSkrZ0/w4dkR6F69hu+5V8+233loxXr1VFOXTuVbbxceee5awuql0RHH3KEpbOaMsNew3ltsN35agr5xKLwfL5bWjfrYJ//WYgt39vMM/9bgDl66OvxCTbftGYVMsLqZm5ljS6dlJDv3FZQIfw1jHufs1NGhAOij4IeKGep08HVrn7SGAkcKaZDQR+Crzm7kOB3YHJjbzMMOBiYBdgW2AfM2sD/B04wt33BQoayHiWmX1sZh+XU7alZltWZ8e1eiaB9LiZIdu0q+KKu6bx92v7s35tNnltqjjh/IU8ess2zX/tFlZ/9tbP0RzJlrkl8rRpU8HlV77PvXcPZcP6VhgbVV++Ou9j2tud6b3Lei77cDIXvjSF/141gNI1MaorjUVftWevk5Zy4UtfkdOumrfu7pX4zI1Itv2iMamWF1Izcy1p1IlpaEzMYne/ptWSpI+2ZjYZGAB8AvyvnjaHAruZWc3hpc7ADsAk4EEzywH+4+6TG3mtj9x9AUDca64FZrn77LDNE8BZ9a3s7vcC9wJ0jnVvcDcsKcqloNemqkp+r3KWLa39IVOyOJf8XmXUnJWf37OcZUuCNlnZ1Vxx13TefCGf91/rBkCv/mX03KaMu176cmP72/87hYu/P5gVJa07PqZkcQ4FvePfXwXLiqIZYNxUyZa5pLgt+QXrN+XJ38DyZW2bvH5WVjWXX/U+E97ox/vvtk7HtnOvClbFVQ9XFeXSqbB29fDTZwsYc84izKD7gDK69i2jeGZbuvQpp1PPcvoOC4YKDjliOW9vNti99SXbftGYVMsLqZk5XTVUiUmfiyu0rg1hJaU/kEs4JqYOAy4Mx84MdfeB7j7O3d8GxgALgUfN7JRGXiu+fFJF0ClNyP/btC860HtAKYXblJKdU83+Ry1n4uu1xw5MHN+Fg35QAjg7DV3DujVZrCjOBZyLb5jN/Jlt+fcDm76pzpnajhP3HM6pY4Zx6phhlBTlcuHRQ1q9AwMwdXI7+gwsp7BvGdk51Yw9diUTxyV2UOnWSrbM06Z2o3eftRT2XEt2dhVjxs5j4gdN/VB3Lr5kEvPndeLf/xqU0Jzx+uy2lpI5eSyfn0tlufHFf7uz88Era7Xp3LuMme8HP9c1xdmUzGpDt35ldCyooHOvcopnBoM5Z77fmR7bJ34cT2OSbb9oTKrlhdTMHK/mStYteYtKQ5WYg1otRRpy91Vm9kvgeTO7u87TrwHnmtkb7l5hZjsSdFzygYXufp+ZtSc4O+wfzXzpb4FtzWyAu88Bjt+6dxKorjLuvnoA1z0ylayYM+6ZAuZNb8eRP10CwMuPFzLpzS6MHLuSB9/8nNLSGDf/dlsABo9Yy8E/LGH2t22548Wg6vLIjX2ZNKFLS0RrEdVVxp2X9+H6x2cRy4JxT3Zj7rTkPtMg2TJXV8e4+449uO7/vU0s5ox7bSDz5nbmyKNmAPDyi9vTtesGbr3zddq1q6Daje//cDpnn3E4Aweu5KBD5jJ7Vmduv2ccAI88uCsff5TYwzNZ2XDMn+by0Ck74dUw/MfFFO64gQ//GRyF3eukYg68cBHPXrottx4+BHc47Hfzad+tEoCj/zSXp3+1HVXlRtd+ZRz311kJzdsUybZfNCbV8kJqZk5X5il1IC/5mdlad+8Q9/i/wNPAO8CL7j7EzGLAdQRnehnBZRy+H95+A1QQHBY6xd1n1znFeq27dzCzscCl7n5U+Dp3AB+7+8NmdjTwV6CE4FT5Qnc/qaHcnWPdfVSbI1vop9A6qktLG28kWyVrcOtVRVrKtS8+GnWEZvvDwD2jjiBJ5kMfz2pf3uKV9TZ9+nr/c37deMNmmnblrz9x9xEtvuFGNPsCkNKw+A5M+PjouIdDwmXVwB/CW7xHwlvdbQ6ou313nwBMiFt+Qdwqb7r7TuHZSncCHzf/nYiISFqKqHZhZocDtxKcOHS/u9+whXYjgYnA8e7+bEPbjP58QEmEM8OBvl8RDBr+e7RxREQkk4Vn7N4JHEFwRu2JZrbLFtr9H8Gwi0apEpOG3P1m4Oaoc4iISJKJbiDunsAMd58FYGZPAscCX9dpdyHBRaZHNmWjqsSIiIjI1sqvmXcsvNWd2qMPMD/u8YJw2UZm1gf4AXBPU19UlRgREZFMkphKTEkjA3vrG6RcN8ktwO/cvcrqm1GwHurEiIiISKItAPrGPd4GWFSnzQjgybADkw8caWaV7v6fLW1UnRgREZFMEs2YmEnADuEldhYCJxBcamcjdx9Yc9/MHiaYluQ/DW1UnRgREZEMYUQzsNfdK83sAoKzjrKAB939KzM7J3y+yeNg4qkTIyIiIgnn7i8DL9dZVm/nxd1Pbco21YkRERHJJGk0Ub9OsRYREZGUpEqMiIhIpoj4qtMtTZ0YERGRTJJGnRgdThIREZGUpEqMiIhIJlElRkRERCRaqsSIiIhkEA3slbTjgHsa7dnSImxDWdQRmq0wqzzqCCLJLY3+1OtwkoiIiKQkVWJEREQyhaNKjIiIiEjUVIkRERHJIOk0sFeVGBEREUlJqsSIiIhkkjSqxKgTIyIikkF0OElEREQkYqrEiIiIZBJVYkRERESipUqMiIhIpkizye7UiREREckQFt7ShQ4niYiISEpSJUZERCSTpNHhJFViREREJCWpEiMiIpJB0mmyO3ViREREMkkadWJ0OElERERSkiox0mTDx6zk3KvmEYs5rz5VwNP39K7Twjn3qnmMHLuSstIYN126LTO+ag/Ar/5vFnsduJKVy3I45/BdN64xcOf1/PK62bRpV82ShXn85eLtWL82qxXf1SYjxq7mnGsXkRVzXnmiG0/fURhJjuZIhszD91rCWRd9SSwG417sxzOP7VinhXP2RV8yYvRSykqzuPn6Ycyc1gWAY348k8OOnosZvPZCf55/Zrtaa/7wxBmcfv5XnPi9w1m9Ki8h+b+c0IUnrt4WrzL2O2EJR56/oNbz61dncf9Fg1i2KI/qSjjs7IXs+5OlAPzvgd68/UQhOIw5cQmHnLEoIRmbKxn2i+ZItbyQmpk3UiWm9ZjZ5Wb2lZl9YWaTzWyvFt7+MWZ2WUtusxmvPdbMVoXv6wsze93MeoTPnWpmd0SRqz6xmHP+NXO54tQdOevQXRl7zDL6bb+hVpuRY1fRe0Appx2wG7f+fiAXXDdn43P/+1c+V5w6aLPt/ur/zebBv/Tl3CN25f3XunLcWYsT/VbqFYs551+/kCtOGsiZYwdxwLEr6bdDaSRZmioZMsdizrm//oKrLh3NuT87kDEHL6TvgNW12owYtZTefddx5gkHcftfd+f8Sz8HoP/A1Rx29Fx+feYYLjh1LHvuU0TvbdZuXC+/xwaGjljK0qK2CctfXQX/vGI7fvXIV1w7/lM+fKGARdNqv96b/+hFrx3W86fXPuO3T3/JU9cOpLLcWDC1HW8/UcgV//2cq1/7jM/Hd2PJ7DYJy9pUybBfNEeq5YXUzJyukroTY2ajgaOAPdx9N+BgYH5Lvoa7v+DuN7TkNpvpHXcfGr6/ScD5EWbZokG7r2Xx3DyK5rehsiLGW//tzuhDVtRqM/qQFYx/Lh8wvp3cgQ6dquhWUA7AlI86sWbl5oW/Pttu4MsPOwLw6bud2Ofw5Ql/L/UZNGw9i+bkUjQvj8qKGBOe78Low1ZFkqWpkiHzjjuvYNGC9hQtak9lZYy3X+/DqH2LarUZtd9i3ni1L2BM/aob7TtU0LV7KX0HrGHqV10pK8umuirGl5/lM3rMpk7smRd+yUN3D8YT+K1x1uSO9BhQSkH/MrJznT2PLuazcd03a1e6Lgv34N8OXSqJZTuLp7dluz3WkNe2mqxsGDRqFZ++uvm6rS0Z9ovmSLW8kJqZN/JgYG9L36KS1J0YoBdQ4u5lAO5e4u6LAMzsSjObZGZTzOxeM7Nw+QQzGxHezzezOeH9D81scM2Gw3bD4yseZradmU0Mt3uNma0Nl48N2z9rZt+a2T/jXu8gM/vMzL40swfNLC9cPsfM/mRmn4bP7dTQGw231xFYUc9zR4f5PwurNYXh8qvD15xgZrPM7Jdx65wSVnc+N7NHv9uPf5PuPSsoXrypnF9SlEv3nuW12xSWU7w4d+Pj4sWbt6lr7rR2jDpkJQBjjlxOQa+G2ydK954VFC/alL1kcQ75vSoiydJUyZC5e0EpJUs3VS5KitvSvaD2N9Lu+aUUx7dZ2pbu+RuYO6sTQ4Yuo2OncvLyKhkxegkFPYLq3l77LGZZSVtmz+ic0Pwri3Lp1rts4+OuvcpYuSS3VpsDT13M4hltuWTEnlx16B6ccPUsYjHoM2g90z7szNoV2ZRtiPHFm11Zvjgxh7yaIxn2i+ZItbyQmplr8QTcIpLsnZhxQF8zm2Zmd5nZ/nHP3eHuI919CNCWoGLTkCeBnwCYWS+gt7t/UqfNrcCt7j4SqHtwexhwMbALsC2wj5m1AR4Gjnf3XQnGGJ0bt06Ju+8B3A1cuoVc+5nZZGAeQaXpwXravAuMcvdh4fv4bdxzOwGHAXsCV5lZTthZuxw40N13By6q74XN7Cwz+9jMPq7whkuhVs881XW/IdffpuEJrv/224EcffISbn9hCm3bV1NZEc2E2E15f8kmGTJbfV/BmrBfgDF/bkeefWwHrrv5fa65aSKzZ3SmqsrIy6vk+J9P47H7G+z3t4h6f1518n71Vhf67bKOmz7+iKte/YzHr9yODWuy6L3DBo44dwE3nTSEm08eTN+d15GVFf1Okwz7RXOkWl5IzczpKqk7Me6+FhgOnAUUA0+Z2anh0weE1YkvgQOBwfVvZaOngR+H938CPFNPm9Fxyx+v89xH7r7A3auBycAAYBAw292nhW0eAcbErfNc+O8nYfv61BxO6gs8BPylnjbbAK+F7/U31H6vL7l7mbuXAEuBQoKfx7PhMty93mM07n6vu49w9xE51vCx/JLFORT02vSNNb9nOcvrfGMtKcqtVUkp6FXO8iU5DW53way2XH7KTlx4zBAm/Lcbi+dFM6agZHEOBb03Zc/vVcGyooazRy0ZMpcsbUt+j01jo/ILNrCspPb/YUlxm40VFgjGutS0GfdSfy46fSy/u2Bf1qzOYdGCDvTss57CXuu54+E3efCZceQXlHLrg2/RtVvLjzno2quc5Ys2VU9WLM6jS4/a1cB3nylkj8OXYQaFA0rJ71vK4plBZWm/E5Zw1cuTuezZL2nfpZIeA2uPE4tCMuwXzZFqeSE1M8fT4aRW5O5V7j7B3a8CLgB+FFZA7gKOCysg9wE1fzkr2fS+2sRtZyGwzMx2A44nqGg0R1nc/SqCqktjZYOadWraN+YFaneCatxOUHnaFTibuPfVQK4W3a2mftGB3gPKKNymjOycavY/ehkTX+9Sq83E17ty0A9LAGenoWtZtyaL5cW59W6vRufuQQnWzDnxgkW89M8eLRm7yaZObkefgeUU9g3e39hjVzJxXGIPZWytZMg87dsu9Om7jsJe68jOrmbMwQv58L2etdp8+G5PDjx8PuAMGrycdWtzWLEs2IU7dwl234LC9ey9/2Leer0Pc2d14qSjj+C0Hx/KaT8+lJLiNlx02v6sWN7yHdyBu69hyey2FM/Lo7Lc+Oi/BQw9pHafv3vvMr55rwsAq4pzKJrZloJ+QYdqdUnwwbVsYR6fvtqdvY4pbvGMzZUM+0VzpFpeSM3M6SqpT7E2s0FAtbtPDxcNBeay6UO8xMw6AMcBz4bL5hBUbz4Kl8erORTT2d2/rOclJwI/Ap4CTmhCxG+BAWa2vbvPAE4G3mrCeluyLzCznuWdgYXh/Z83YTvjgX+b2c3uvszMum2pGtNU1VXGXVf158//+DY4lfaZAuZOb8eRPw1ONX358R589GZnRh6wkgcnfEHZhhh/++3AjetfdusMdhu1hk5dK3n0/c947JZteO3pAsYevYyjT1kCwHuvdmPcM/lbE3Or3t+dl/fh+sdnEcuCcU92Y+606M80aUgyZK6uinH333bj2r99QCzm/O+lfsyb3Ykjjp0NwCvPD2TSB4WMGL2E+596feMp1jX+8OeP6NSpnMpwO2vXNNzpbWlZ2XDStTO5+eQhVFfBvscvoc+g9Ux4NOiIjT25iKN+OZ8HL9mBKw8Zhjsc9/s5dOxWCcBdZ+/E2hU5ZOU4J107k/Zdqlo1f32SYb9ojlTLC6mZuZY0OvRlnsQH8sxsOEEVogtBhWUGcJa7l5jZdQQdjTkEZyzNdferwwG0TwNrgTeAn7n7gHB7hQSdgWvd/U/hslOBEe5+gZntADxGUMl4KXytPmY2FrjU3Y8K17kD+NjdHzazg4AbCTqEk4Bz3b0sHFA8Isw6ArjR3cfWeX9jgeeB2eFrrgLOcPdpdXIdC9wcZp8IjHT3sWZ2NbDW3W8MtzcFOMrd55jZzwkOPVUBn7n7qQ39rDvFuvuovCMa/g9JMl5W1ngj2SrZ2w6IOkKz/X3CY1FHaLbT++0bdQRJMh/6eFb78hYfJNiuR18fdNyvW3qzTL7715+4+4gW33AjkroT09rMrB2wwd3dzE4ATnT3Y6PO1RrUiZH6qBPTOtSJkboS2YnZ6Uct34n57J5oOjFJfTgpAsOBO8LTnVcCp0UbR0REpAVFfEp0S1MnJo67vwPsHnUOERERaZw6MSIiIpkkjSoxSX+KtYiIiEh9VIkRERHJEEa0k9O1NHViREREMkkadWJ0OElERERSkioxIiIiGcTSaH44VWJEREQkJakSIyIikik02Z2IiIikqnQ6O0mHk0RERCQlqRIjIiKSSVSJEREREYmWKjEiIiIZJJ3GxKgTIwBYdjZZ+d2jjtEslQsXRR0h7ZX37Rp1hGZ7eOWIqCOISCtRJ0ZERCSTqBIjIiIiKcfT63CSBvaKiIhISlIlRkREJJOoEiMiIiISLVViREREMoSRXmNi1IkRERHJJJ4+vRgdThIREZGUpEqMiIhIBkmnw0mqxIiIiEhKUiVGREQkUzhpdYq1OjEiIiIZxKqjTtBydDhJREREUpIqMSIiIpkkjQ4nqRIjIiIiKUmVGBERkQyiU6xFREREIqZKjIiISKZw0uqyA+rESJMNH13MWZd8QyzmjHt+G555ZLs6LZyzL/mGEfsUU1aaxc1/2pWZUzvTp/9aLrt+8sZWPXuv57F7d+D5Jway70GL+elZM+g7YC2/OnVvZnzTuVXfU7wRY1dzzrWLyIo5rzzRjafvKIwsS1MlW+YRuy/gvFM+IhZzXnlzB556Ybdaz/ftvZJLz36P7Qcu46Gn9uDZl4ZsfO6HR3zFEQdOxx3mzO/KX+/Zh4qKxP+JWv5ujFn/l41XQ88fVtH39Kpazy94KIulL2cB4JWwfrYx6q0ycjrDR4fnkdXOsSywLBj2ZHnC8zZFsu0XjUm1vJCamWvocJK0GDO73My+MrMvzGyyme3VAtucYGYjWiJfjVjMOfe3X3HVRSM49yf7MebQxfQduKZWmxF7F9O73zrO/OEYbr9+MOdf9hUAC+d24MKT9uXCk/blopP3oawsi/ff7AnA3Jkd+fNvhzHls24tGbfZYjHn/OsXcsVJAzlz7CAOOHYl/XYojTRTY5Itc8yqufAXH/KH/zuEMy79PgfsPZt+fVbWarNmbR53PrIXz744pNby7l3X8f3Dv+H8PxzFWb/9PrGYc8Do2QnP7FUw8/psBt9dwfD/lFP8ShbrZlqtNtv8ooo9nilnj2fKGXBRJZ2HV5MT19fe7YHguWTpwCTbftGYVMsLqZk5XakTEyEzGw0cBezh7rsBBwPzm7huViKz1bXj4JUsmt+eooXtqKyM8fb/ejFq/6W12ozafylvvNQHMKZO6Ur7jpV07V77F3v3kSUsXtCO4qK2AMyf04GFczu01tvYokHD1rNoTi5F8/KorIgx4fkujD5sVdSxGpRsmQdtX8Kioo4ULe1IZVUWEz4YyN4j5tVqs3J1W6bNyqeyyjZbPyurmrzcKmKxavJyK1m2ol3CM6+ZYrTp57TdxonlQMHhVSx/c8t/FotfyaLgiOSeKSzZ9ovGpFpeSM3MtXgCbhFRJyZavYASdy8DcPcSYGcz+3dNAzM7xMyeC++vNbNrzOxDYLSZXWlmk8xsipnda2bxnww/NrOPzGyame23tUG7F5RSsqTNxsclS9rQvaB0szbF8W2WtqF7j7JabcYcupi3Xuu9tXFaXPeeFRQvyt34uGRxDvm9KiJM1Lhky5zfdT3Fy9pvyrOsPfld1zdp3WUr2vPsi0P45x3P8NTdT7FufS6ffNknUVE3Klti5BVu+gucW+iULd28gwVQtQFWvBcj/5D4w03Ol2fn8tnxuSx+tlW/V2xRsu0XjUm1vJCamdOVOjHRGgf0DTsad5nZ/sAbBB2ZgrDNL4CHwvvtgSnuvpe7vwvc4e4j3X0I0JagqlMj2933BC4Grqrvxc3sLDP72Mw+Lq/e0GBQq+/vulsT2sQFyq5mrzFLeXd8zwZfKwr1ZU/2sW/JlrnePE1ct0P7MkaPmMfJvzyOE847njZ5FRy078wWzddk9fdhWP5WjE5Dax9K2v0f5ezxdDmD7ypn8ZNZrPp4Cyu3omTbLxqTankhNTPXMIIxMS19i4o6MRFy97XAcOAsoBh4Cvg58CjwMzPrAowGXglXqQL+FbeJA8zsQzP7EjgQGBz33HPhv58AA7bw+ve6+wh3H5Eba9tg1pKlbcgv3FR5yS8sZVlJ3mZtCuLb9ChlWfGmNiP2Lmbmt51Yubz2esmgZHEOBb03jWnI71XBsqKcCBM1LtkyFy9vR0H3dZvydF/X5ENCewxZTNHSjqxa04aqqhjvTurPLjsubXzFrZRX6JQt2fSJVL7EyCuo/y9y8atZFBxRe9BvXo/g39zu0P3AatZMif5ParLtF41JtbyQmpk3ck/MLSLR/8ZlOHevcvcJ7n4VcAHwI4LKy8+AE4Fn3L0ybF7q7lUAZtYGuAs4zt13Be4D2sRtuuY4ThUtcBbatK8706ffOgp7ryc7u5oxhyzmw7d71Grz4ds9OPB7CwFn0JAVrFubzYplmyKNOWwxb41LvkNJAFMnt6PPwHIK+5aRnVPN2GNXMnFcdGdKNUWyZZ46M58+PVfTs2AN2VlVjB09mw8+6dukdZeWtGfnHYrJy60EnGFDFjNvYZeE5gXoONgpnWuULjCqK4KOSrexm495qVwDqz6O0f2ATc9VrYfKdZvur/ggRrvto/86nmz7RWNSLS+kZuZ0pVOsI2Rmg4Bqd58eLhoKzHX3RWa2CLgCOGQLq9f0DkrMrANwHPBsorJWV8W4+y+7cO1tk4hlOf97YRvmzerIET8MBm6+8lw/Jr1XwIh9irn/328Fp1hfs+n02ry8KobtWcId1w+utd3RY4s459Kv6dy1nKtv/phZ0zpx5S9HJuptbFF1lXHn5X24/vFZxLJg3JPdmDutTeMrRijZMldXx7jj4VH8v9//j1jMeW3C9sxd0JWjDv4WgBdf34munddz559fpF3bCtzhh0d8zRm/+T7fzizgnQ/7c9f1L1BVHWPmnG68PH7HhGe2bNjuD5VMOTcHr4LC71fRfntn8dPB+JZePwkqL8veyKLL3tVkxRWWypcb31wcfPv2Kig4oopu+0Y/6DfZ9ovGpFpeSM3M8dLpFGvzVDmQl4bMbDhwO9AFqARmAGe5e4mZnQBc7O6j4tqvdfcOcY+vA04A5hCc1TTX3a82swnApe7+sZnlAx+7+4CGsnTOLfS9C09oybeXcJULF0UdIe1V7z8s6gjNtv/tH0Qdodne2S11PgCldXzo41nty1t8kFXHLtv4sP0vaunN8s4Lv/3E3Vt0ao+mUCUmQu7+CbD3Fp7el+AQUXz7DnUeX0FQram73bFx90vYwpgYERHJQGlUu1AnJgmZ2SfAOuCSqLOIiEh6SafDSerEJCF3Hx51BhERkWSnToyIiEimcKA6fUoxOsVaREREUpI6MSIiIpkkomsnmdnhZjbVzGaY2WX1PH9SeDHkL8zsfTPbvbFtqhMjIiIiCRVetPhO4AhgF+BEM9ulTrPZwP7hBZGvBe5tbLsaEyMiIpJBIjo7aU9ghrvPAjCzJ4Fjga9rGrj7+3HtJwLbNLZRdWJEREQySWImuc03s4/jHt/r7vGVlD4Ek7LWWADs1cD2TmfTdQO3SJ0YERER2VoljczYW9/sw/X2pszsAIJOzL6Nvag6MSIiIhkkosNJC4D4K8JuA2x27Rgz2w24HzjC3Zc1tlEN7BUREZFEmwTsYGYDzSyX4Lp/L8Q3MLN+wHPAye4+rSkbVSVGREQkUzTjlOgWfVn3SjO7AHgNyAIedPevzOyc8Pl7gCuB7sBdZgZQ2dhFJdWJERERyRAGWGIG9jbK3V8GXq6z7J64+2cAZzRnmzqcJCIiIilJlRgREZFMUh11gJajSoyIiIikJFViJFBVRfXqNVGnkCTz/D//HnWEZjtu98OjjvAdLI86gGSQqMbEJII6MSIiIpkiorOTEkWHk0RERCQlqRIjIiKSMTxR106KhCoxIiIikpJUiREREckgEV07KSFUiREREZGUpEqMiIhIJkmjMTHqxIiIiGQKB9OMvSIiIiLRUiVGREQkk6TR4SRVYkRERCQlqRIjIiKSSdKnEKNOjIiISCZJpwtA6nCSiIiIpCRVYkRERDKJKjEiIiIi0VIlRkREJFM4kEaT3akTIyIikiEMT6uBverESJMN328F51w+i1jMefWZQp65r2+dFs45l89i5P4rKCuNcdNlOzLz6w7k5Fbz139+QU5uNVlZ8O5r3Xns9v611vzRaQs443dzOH7UXqxekdN6byrOiLGrOefaRWTFnFee6MbTdxRGkqM5ki3zp2925oGr+lFdZRx8YjE/umBxrefXrszijksGUjS3DTl51Vxw02z677QBgNsvGcjHr3ehc34Ft42fktCcw/dZxtm/m04sC157rhfPPNC/Tgvn7MumM3K/5ZSVxvjbFTsz85uOADz06gdsWJ9FVZVRXWVcdMIIAPY9dCknnTubvtuu51cnDmf6150S+h4akmz7RWNSLS+kZuZ0pDExzWRmPzCzyXVu1WZ2kpk924T1f2xm35jZmy2QZayZvbi1bZoiFnPOv3ImfzxjMGd/bw/GHlVMv+3W12ozcswKeg8o5fRDh3PbH7fngqtnAFBRblz28105/9g9OP/7Qxm+3wp22n31xvXye5YxbO+VLFmYt7Uxv7NYzDn/+oVccdJAzhw7iAOOXUm/HUojy9MUyZa5qgruvaI/f3x0Gre9+SXvPt+d+dPa1Grz7O29GTh4Pbe8PoWLbp3FA1f12/jcgT8u4crHpiY8ZyzmnHf5NK48b3fOOXZP9j9iCX23XVerzYj9ltOn/wbO+N5e3PanQVxwRe1cl502lAt/PHJjBwZg7vT2XPerXZnySZeEv4eGJNt+0ZhUywupmbkW95a/RUSdmGZy93+7+9CaG3AX8A7whLsf14RNnA6c5+4HJDJnS9txtzUsmtuGogVtqKyI8dZLBYw6aFmtNqMOWs74//QAjG8/70SHTlV0LSgHjNL1WQBkZzvZ2Y67bVzv7N/P4oG/Doh0AqZBw9azaE4uRfPyqKyIMeH5Low+bFV0gZog2TJPn9yBXgPK6Nm/jJxcZ99jl/HRuK612iyY3pZd9w06sNtsX8rSBXmsLA4KwoNHraFjl8qE59xx19UsmteWogVtqayM8fYrhYw+oKRWm1EHlDD+hZ6AMfWLzrTvWEnX/LIGtzt/dnsWzmmXwORNk2z7RWNSLS+kZuZ0pU7MVjCzHYErgZOBfmY2JVx+qpk9Z2avmtl0M/tLuPxKYF/gHjP7q5kNMLN3zOzT8LZ32G6smU0ws2fN7Fsz+6eZWfjc4eGyd4EfxmVpb2YPmtkkM/vMzI5tyfeaX1hOcdGmSknJkjy6F5bXatO9sIySotxNbYpyyS8M/vDHYs4d//mMJ97/kM/e78LUL4LS/F4HLqNkaS6zp3ZoybjN1r1nBcWL4rIvziG/V0WEiRqXbJmXL84hv9emD/ruPctZtji3VpsBu6xn4ivdAJj2WXuKF+Rt1ibRuvcoo6RoU4Uo2Jdrd1Dye5Rttr/n9wjauMN1f/+cW5+axOHHLWqd0M2QbPtFY1ItL6Rm5lrSqBKjMTHfkZnlAI8Dl7r7PDMbUKfJUGAYUAZMNbPb3f0aMzswXOdjM2sHHOLupWa2A/AEUFOfHgYMBhYB7wH7mNnHwH3AgcAM4Km417sceMPdTzOzLsBHZvZ6I+/hLOAsgDbWvpE3XM+yOvut1dOmpuJSXW1c8P1htO9YyR/v/Ib+O6yjaH4bTjhnPpefNqTh124F9Wdv/RzNkWyZ63vpuhl/eP4iHriqP786dDD9d9rAtkPWEctu3dBN+rnZ5pk8/CW49JQ9WF6cR+du5fz53sksmN0u8kNI8ZJtv2hMquWF1MycrtSJ+e6uBb5y9ye38Px4d18FYGZfA/2B+XXa5AB3mNlQoArYMe65j9x9Qbj+ZGAAsBaY7e7Tw+WPEXZCgEOBY8zs0vBxG6AfDXD3e4F7ATpn5Tf4K1hSlEtBz03fVvMLy1i2NLdOmzzye26qzuT3LN+szbo12XzxYWdG7LeCT97tSs9tyrjr+c/C9mXc/txkLv7x7qwoad1v5yWLcyjoHZe9VwXLiqIZYNxUyZa5e68KShZvql4sK8qlW8/a1bp2Hau58G+zgeCP/tmjd6ewb8OHaVpayZI88ntuGr+QX1jG8qV5ddq02eL+vrw4aLtqeS4fjC9gxyGrk6oTk2z7RWNSLS+kZuaN0uwUax1O+g7MbCzwI+CCBprF/2Wuov4O46+AJcDuBBWY+E/uLa2/pc6GAT+KG6/Tz92/aSBfs0z7siO9B2ygcJtSsnOq2f97xUx8o1utNhPf6MZB318KODvtvpp1a7JYUZxL564VtO8YjHXIzati2N4rmT+rHXOmtefEvffi1INGcupBIykpyuPCHw5t9Q4MwNTJ7egzsJzCvmVk51Qz9tiVTBzXudVzNEeyZd5h97Usnp3Hknm5VJQb7z7fnZGHrKzVZt2qLCrKg6+x/3u8gMF7raFdx9b9izptSkd6999AYZ8NZGdXM+aIJUyckF+rzYdvduegY4oAZ9Buq1i3NpsVJXnkta2ibbtgX85rW8WwvZczd0YjVcxWlmz7RWNSLS+kZuZ45t7it6ioEtNMZtYVeAj4qbuv2crNdQYWuHu1mf0cyGqk/bfAQDPbzt1nAifGPfcacKGZXejubmbD3P2zrcy3UXWVcfc123Hd/VPIyoJx/ypk3oz2HHlCcArty0/2YtJbXRm5/woe/N8nlG6IcfMfdgCga49yLr1hGrEsxwzeeTWfjyZ0a+jlWl11lXHn5X24/vFZxLJg3JPdmFvnzJpkk2yZs7LhzGvn8qeTdqK6Gg46vph+gzbw6qMFABx+cjHzZ7Tltou2JZblbLPDBi64cfbG9W86fzu++qAjq5dnc8aIoZxwyQIOPrFkSy/3nVVXxbj7+h257p7PiWU54/7di3kz23PkjxcC8PIzfZj0TndGjlnOAy9PpKw0i5uv2AmArt3LueKWL4P3m+VMeLmQT97rDsDoA4s59w/T6dy1nKvv+oJZ33bgj+cMbfH8jb+/5NovGpNqeSE1M6crcx3IaxYz+z1wBTC9zlNPACe7+xAzOxUY4e4XhOu8CNzo7hPMbAKbxsTsAPwLWA+8CVzo7h3CSs+l7n5UuP4dwMfu/rCZHQ7cApQA7wJD3P0oM2sbLt+boCozJ1xea1tb0jkr30d1OOa7/2AiUL1ma/uQ0ph/L/go6gjNdtzuh0cdodmqli2POoIkmQ99PKt9eX2jEbdK53a9ffQOp7f0Znnti+s+cfcRjbdsWerECKBOjNRPnZjWoU6M1KVOTNPocJKIiEjGiPaU6JamToyIiEimcNKqE6Ozk0RERCQlqRIjIiKSSTRPjIiIiEi0VIkRERHJIFFOTtfS1IkRERHJJGnUidHhJBEREUlJqsSIiIhkCgeqVYkRERERiZQqMSIiIhkjvWbsVSVGREREUpIqMSIiIpkkjSox6sSIiIhkkjTqxOhwkoiIiKQkVWJEREQyRZqdYq1OjACwunpZybjVD81NwKbzgZIEbDeRUi1zwvJ27JOIrQIJ/Rnfm5jNar9oDcq8Sf8EbDPtqBMjALh7QSK2a2Yfu/uIRGw7UVItc6rlBWVuDamWF5S5dTh4+lzGWp0YERGRTKKBvSIiIiLRUiVGEi1hAxQSKNUyp1peUObWkGp5QZkTL80G9pqnUVlJREREtqxzbqHv3fPEFt/uq/Nv/SSKsUGqxIiIiGSSNCpeqBMjIiKSSdKoE6OBvSIiIpKSVImRhDKzA4HB4cMp7v5mlHmaIpUym9lqwAiG621cHP7b3t31RaUF1Pk5twM2sOln3t7ds6LKVpeZ7QGsdfdp4eP2wKHAYnefGGm4LUjRzJcBf3f3FfU89313/0/rp2oKVyVGpDFm1tPMPgCuAgaEt6vM7H0zK4wy25akYmZ37+TuHcN/OwF7A7cC04BHo023OTNbbWZrwn8r4+6vNrOqqPNtSfzPGfiizs/886jz1XE7UA1gZga8C5wK3GRml0eYqyGpmPly4BMzG1vPc1e1bpTMpUqMJMqtwP3u/kD8QjM7E7gFaPnh8VsvFTNjZn2A04BjgDnAE8C17l4WZa76hB/6AJjZp+6+R/zjaFI1nZl1BAaaWZa713S6cqLMVI/O7j4jvL8f0NbdjzWzbGAy8OfIkm1ZKmaeDpwL/MPM/g1c4e6V4XPJW+pwoDp9ZuxVJUYSZXjdzgCAu98HjIwgT1OkYmaAHwK/BO4EfubuzyVjB6Y+4YdUjWTrDNRiZkOBF4GPgSfM7GwzexiYFWWuelTG3T8QGA8QfsAma7UrFTPj7h8Cw4BC4AMz2zHiSE3j3vK3iKgTI4myoYHnSlstRfOkYmbc/XZgLLA9MNHMHjGzw80sacZpbME7wLNhZ+Ax4OuoA22JmT0PPA88SzBW435gV2AqcHKE0eozw8yuNbMfAWcBTwGYWTeSt0OQipkNwN3Xu/svgL8A48PKrTW4prQYdWIkUeaY2WYTH5nZbgSHPJJRymU2s/3NbH+CK+n+D7gImAJcDxRFma0JLgFeIegMfAz8PNo4DXoa2Nbdb/fAOHe/wN3/n7uvjjpcHWcBnYFTgMvc/e1weS5wdmSpGhaf+ffu/nY4wD6P5M1cq/zg7s8Ao4CfArtFkqip0qgSozExkigXA33qWd4xfC4Z/Qc4MjwzIt5+BN/Ck9ElW1i+ILwllbDDFe9b4Ft3f8vMhgOfRBCrKd4FtgnGnG7i7nPNrJe7L44m1ubcfbmZ/REojT+s6O5FJFnHtuYK0O6+HPilmfUEfmFmXxPsFz8EkuZnW8eTZta/7kJ3P8DMknL8XDpSJ0YSZSXwPTMbCPyT4MyDtu7+XqSpGnZs+O8Igm+AexIMKlxDUB6+L5pYW+bux0SdoZnq63QZ8BbBYZlk7cT8t55lRlBFuhX4SevG2TIzuwY4M7hr5xOML7nQ3a+NNlm91prZ7QSdxJOA3gRn1e3n7ssiTda4n4W3eDX7xA8IBtgnIde1k0QaY2bvA5OAHkAJ8HvgP+5+cKTBmsHMegG3ufuPo86yJWbWBvgjcBhBeXsc8Gd3Xx9pMImMmU0HhgDdgefcfZSZfejue0UcbTNmlkPQETgT6EtwFtIT7r4q0mBprHNOge/d5Uctvt1XS/6uaydJWsl294vMLAZ85u5rzaxL1KGaqYjgwyCZ3QJUEJz+/S/gK4I5N06PMNMWmVm982e4+59aO0tzhHOXnEHtzuID7p6M56rOBnLcfZGZtQuXtY0y0Ja4ewXwEPCQme1EMDfMp2Y2EXjI3V+PMl9DzKzeMVzu/khrZ2kWh+Tcbb8bdWIkUSab2QHu/qaZVZtZd5L/FNrb2HRWQRYwlGDAaTLb2913AzCzKnd/3MwuijpUA9bE3c8Dvkdwhk+yuw7YBbiLoJP4GXAD8NsoQ23BfILTfZ8FuprZP4D3I87UKHf/FrjMzP4AHA6cAyRtJwYYHnc/DziIYOLD5O7EQFodTlInRhJlH+AMM5tLcEhpIlsehJos4jsslcBj7p7sf/xrjTQ1s84k8e+1u/8t/rGZ/ZXgrKpkdzSwh7tXmtkGd7/ezD6MOtQWzA1vEHS4vnL3lyLM0yxhdevl8Ja03P2X8Y/NrAPwTERxMlbS/rGTlHdE3P1Sd18aWZImcvd/hJ2AWmd1JLnZZjbU3ScDXYCPgEsjTdQ8nQnGQiQ7i5uNFTPLJfj2nXTc/ZqoMzRXnWtTtSH4bFrn7h0jDdY8FcC2UYdokjQaC6tOjCRKfIWgbd1TEd19LknGzK4lGPeQCmd1AJudnXQ4MM/dG5q0L1Jm9gWbPqyyCGY6TerxMKGlZraDu08HOgHvEcyQnHTMbBBBR3YAcX/j3f2AqDI1ps7lKIxgFuqknmvFzF6g9uHnXYAno0uUmXR2kiSEmU0h+KWeS/CL3o9g7EMFwX63a4Tx6pVKZ3XUSLXBhWbWL+5hJbAk7hpESSu8ZlKVu683s4OB6cnYEQcws8+BuwkOj24cwenuSX9tqnhm9oG7j446x5aY2Zi4h5XAXHdfGFWepuqcle+jO7T8zAyvrX5IZydJWnkbOD28tghmNgo4x91PjTRVw1LmrI448YML2wOHEIw/SspOjLvPM7NhwBiCasx7JO/cMPF+AEwA5gFlwI/M7PFwArlks8bd74k6RHPVcyj3EjOLJekZYMTNhLyRmf0xmSu3G6VR8UKXHZBE2aemAwPg7hMJLpSWzGrO6riKFDmrw91/GXc7nWCirTZR59oSM7uY4JTarkA34EEz+3WkoZrmN8DCcEbZBwnGwzwVbaQtesPMLjazPmbWveYWdaiGhBP0fQvMNbMfhdMxHJSsHRgAM3vDzN6MvwG/NbNXzOy4qPNlCh1OkoQws2eAFWyatfJnQEd3T5qZTesysyvjHpaSYmd11DCz14HDkvEwjZl9CYx099LwcRtgUjIeXoxnZp+6+x5mdjrQx92vqVkWdba6zKy+q2qbuw9s9TBNtIVDuR+5+54RR9siM6v7f+/A4wSnht/t7ru0fqrGdc7K91Ftv9fi2x237h86nCRp5WSCX+YLCMbEvEswx0bSStGzOja7dgvBRHfbmFkyDqCuGdBbI4s6F9JLUmvC+XdOB04OB58m5d9Pd0+NM2Rqq+9QbtJWFKH+MUZm9kJ4HbBkvd5T2knKX0JJfe5eamZ3EkxW5cDU+FNUk5GZvUGdeVcguc/qoP5r+tSouY5LMrkPmGhm/w4f/xC4N8I8TXUywYVL/+Lun1twkdBzo43UODPrTXBdpxPcfVTUeRqQchP0mdn2wHnAKuBmoJzwi5q7HxRhtEZEe9XplqbDSZIQZrYb8CxQTFAmnkJwunLSniFRpzzcHjgeWOXul0cUKS2Z2e4EVwYHeMfdP48yT7oxsx7Aj4ATCA7PPAs85e7fRBqsAal4KDc8C+xhoBfQk2B6htfdfUxD60Wtc6y7j8o7ssW3O670MR1OkrRyO3CKu080s0+BYwiu7TM20lQNqKeD9Y6ZTYgiS1OZWR7Bt8Gas33eBe5M8sn65hNXQTKzJwguELoiWS/8F44z2axKR/AzT7bxJgsIBh1f4O5fRh2mKVLxUC7BZHw3QzD/kbuXxx0Kk1aiTowkSufwjCQI/sgvC0vwSS881dPdfTXwvJllJeMg2dA9BHNU3Bo+PplgjpDTIkvUADN7jOCSFPHXUNqOoFp3F0H2ZNTq3zC3wjUEVcR7zOwp4OkkPRV8oy0dyq2RpId0J5jZL4B/AFXh4aXUkLwnfTWbOjGSKFlmlh2Og4mZ2U+AkqhDNSQ8nPQgwVT4ZmargF8kcQcGYHjNBSBDE8JZcZPVbnWrFsl6lk88d18edYamcvfrgOvMbDDBIaUJZrY4STsCNeIvlZFHMFaqCng6mjhNcgHBYee/E4yHeSJcJq1InRhJlFuAHYGvgUXAYcCpEeZpigeB82ou+mhm+4bLhkYZqhEVZraju0+DjVPOV0ScqSGv1LMs6S8AWefaPhsXu3tHM3vX3feNKNoWuftXwB+BP4YTDCateg7lfhDOln1ZJIGaIP5SCanEAddVrEUa5u4PxN0/oqG2SWRd/FWr3f1dM1sXZaAm+BXwmpnNJ/j7NAA4JdJEDXD33zVlWbJp6AMr2TowW7oUBfCZmR3t7g2d0RaJOpPxZRHMRN05ojhNYmb717fc3d9q7SyZTJ0YSYgUPV35NTO7Afhn+Phk4L2auViScM4V3P1tM9sRGBQumkZqzLuSUlLsA2t4PcuM4FIUO9PwaflRmcSmSlclwTXXTo80UeMuibufB+wJTAaS+W9ccHp1Go2J0SnWkhCpeLpyI2NJkvWilV0J/mh2jFt8DXAlMFmnL7eM8IrFNTZ+YCV5p1xakZn1Am5z9x9HnaUhnayb7xU7pMW3+3r1042eYm1mhxOchJAF3O/uN9R53sLnjwTWA6c2Ni2HKjGSEFs4XfnNSMI0UZ0BsqniNYJxR6vjlrUhOJtmKaBOTAtw91qX/a35wIooTr3CMVyfAH0IJl8bRVDd+AC4yN3ruxyBtJwigrPspB5mlgXcSXCR2gXApHCG46/jmh0B7BDe9iI4W3GvhrarToy0pheS+XTlFJ1zJbvulcHNbF93vzCiPA3a0gBZgr9Fee6eKhelTcYPrDvcfWh4GvvfgGPD5ccRHCIdHVmyNGRmt7HpkHkWwQkAH0cWqDmiOZy0JzCjpjNtZk8S7KPxnZhjgX94cIhoopl1MbNe7r7FyzioEyMJ0cCYmJsjiNNUKTXnSuj/NXFZUqg7QNbMOgDnA2cBz0USqglS5ANr4++bu8efmvy0mV1ST3vZOvH//5XAY/EnBiSrNax47XV/Nj8Bm25jZvE/k3vdPf6SIn0IJrqssYDNqyz1tekDqBMjrW5L8z4ks1SbcwV3f6Ypy5JNOJbnIuDnBFWCkUk+F0sqfGCtMLPjgZfN7CzgSYKK10+BVyNNlobc/R9h9XancNG3UeZpKnc/PKKX3tKM181tU4s6MZIQW5r3IZIwTZdqc66knPC6PpcQXJTwAYLJ79Y0vFb0wg+sNgQfWE5yfmCdBtxEMB6qM3Bj3HMGXBVFqHRlZocRTHQ3J1y0rZmd5e7qMNZvAdA37vE2BHOINbdNLTo7SRJiC/M+3OzuO21hlciZ2RiC01Dj51z5ubtPiDBWWjGztcAygg7MZnPwuPtNrR6qAWZ2McGVt8cSXBZhNsG+sR1wjru/HFk4iZSZfQMc6e6zw8fbAi+5+87RJktOZpZNMAXEQcBCgtPqfxpOyljT5nsEsx4fSXCo6TZ337Oh7aoSI4mScvM+1DfniruXR5kpDf0fm0rGqXAtrZ+7+y1m9ldgP3efB2Bm/QjODEu6TkyKzWmTyoprOjAA7j7LzIqjDJTM3L3SzC4g+L3JAh5096/M7Jzw+XsIfp+OBGYQnGL9i8a2q0qMSGhLM526+yOtnUWSg5l95e6Dzextdx9T57nNliUDzWnTOszseoJq7RPhop8RnLX2HKjT2FrUiZGECCctOoPgmkkOjAMecE/eqSLDM1Bq5BGUPT939x9FFCnthLMf3wrszaY5TC6oqXAkGzN7lGAOng1AT+Cp8KkTCb6JXxRVtqZKlUnYUk2dzuJmT7v70a0WJoOpEyMJYWZ/BnYhGEdwO8Hl6ru4+28jDdYM4em/z6TQtZ+SnpmNBx5i07fXEwlm5Tw4ulRbFg7mPZtNg2VrPZ0KH1ThF4qvNVZD0pE6MZIQ4anJe4THQT9z92HhVWkbnH0xmYSnT37h7oMabSxNYmaT3X1oY8vku9vCnDYz3f3kyEKlobCD+0dqV5v/7O7rIw2WYTSwVxLF3L1y4wOzXIJDNEkrLA/H//HfhU2HD6RlFJvZqcBj4eOTAQ2GbFmpMKdNOriFYAqGE4F/AV8RVJ2T+gSGdKNKjCREeNjgHHefbmYzgeUEMzjeF3G0LQpPsa5RCcx194VR5UlHZrYNwZiYfQk6jO8BF7r7gkiDpZm4SdgcmJrkl85ISWb2Rc3kmKlabU4HqsRIonyfoCMAwZiC6e4+N7o4jXP3t6POkO7CzooGSieQmR1AMO5oLkEnZlsz+4W7j482WXozs87oM7XV6QcuibIB2M3MOhKUXAeEF/z6PTA7GTs0dS5OmENw+Gudu3eMNFgaMbN6Z4119z+Z2dnu/vfWzpSGbgIOjLvQ3nbAM8AekaZKP3PMbKi7Twa6AB9R+3Ir0grUiZFEeZVgXEn8lPI7EUw5/zjBt8SkUs/FCY8kOBVYWk5DlxjYbAZf+U6yajowAO4+08yyogyUjtz9mLiHhwPz3H1DVHkylcbESELUHCOus+xTd0+pb4M6c0ZSjZndT/AF4tFw0c+BCnc/I7pU6WdLMyPX0GR3rUOVGEmUh+tZltQz35pZ/FiNmus96XTJFmRmb1DPlWo1m2yLOpdgHNp54eO3gbuji5O2LmngOQPUiWkFqsSIhMzswbiHlQRXp73P3XUKcAsxs/hKXHvgeGCVu18eUSQRSWHqxIhIpMxsgruPjTpHujCzWdRf7RoYQRyRhNLhJJGQmR0DXEVwrZwLgfkEVy5+MdJg6e95M8ty96qog6SJEXH384AfAD0iyiKSUKrESEKY2WHAUnf/LHzcB/gJsBh4ypNwxwsn5TsO6A380t0PM7OJ7j4q4mgiW8XMPnH34VHnEGlpqsRIovwZOBrAzHKAd4EXgNHhLRmv/rsg7HR9Zmb/L1yWE2UgkeYys/jOSs0Adf2tl7SkHVsSJdfdF4f3DwRWuPtFAGb2ZXSxGjQxHNz7CJBjZqcDyyLOJNJcf427X0kwJ9OPI8oiklDqxEjCmFnM3auBg4A34p5KukNJoZHhv1cBswkm5zspujgizefuB0adQaS1qBMjifIR8JCZfQGcBhwMYGa9CS5JkHTq++NvZpud5SEiIskhFnUASVvnAZ8D/YAfh9cXgWBq+Z9EFao+Zva7epZtY2Z/BKZGEElERJpAZydJwpjZbsBqd58TdZaGmNlsgjEDk4FjgNOBQoJp2x9zd42LERFJQurESEKY2UPAUKADwVV1nwSud/fzGlovCmY2GrgB2J2gUvQzd38z2lQi342u6SOZRJ0YSQgz+woYAnQE3nT34Wb2kbvvGXG0LTKzHYFTCabC/wh4EHg9Gee0EdkSM1tBMKVBzX47BngnfDzG3btEFE2kxakTIwlhZv8FznD3JWb2OTAM+Nzdd404WqPCwbxHAL8Ahrr7DhFHEmmyuleLj3+cileSF2mIzk6SRCkDPjezV4CewHjg2WgjNU1YeXkZeNnMukWdR6SZ6p6wkW9mFu7XWVEEEkkUdWIkUV4IbwCvAF+7+5QI83wn7r486gwizbTSzM4kOIR0NLAWuM3M1gBLIk0m0sJ0OElEJI2Y2fYEg+m3I+jI/IZgrqauwJ3uXhJhPJEWpU6MJISZ7QtcCQwkqPg5wf42MNJgIiKSNtSJkYQws6nAL4FPgOqa5To8I5JYZvbzhp5390daK4tIomlMjCRKkbu/FnUIkQw0vIHnjOACpyJpQZUYSQgz+wtBJ/lfQGnNcnf/JLJQIiKSVtSJkYQwszfqW+zuB7R6GJEMYmbHEFyJfTVwITAf2M/dX4w0mEgCqBMjrSZurgoRSRAzmwkcB/QGfunuh5nZRHcfFXE0kRanq1hLwpnZaDO7GZgVdRaRDLDA3T9z95eAXuGynCgDiSSKOjGSEGY23Mz+z8ymEZxqPZngAosiklgTzezB8EKQOWZ2OqArsUta0uEkSQgzqwL+Cfxak2uJtJ4649FKga+Av7h7cUSRRBJGnRhJCDP7GXACsAvwH+BJd/8o0lAiIpJW1ImRhDKzrsAPgeOBgboitEhimdlV9S139z+1dhaRRNNkd5JQ7r4CeAB4wMwKos4jkgHWxN3PA74HTI0oi0hCqRIjCdHQt0EzO9vd/97amUQykZllA//THE2SjlSJkURZ08Bz61othYh0BvpGHUIkEVSJkYQysy6Au/uqqLOIZAIz+4LgGkkOZAGFwJ/c/fZIg4kkgDox0qLM7CfAS8Bg4H6gE8Ef07XAae4+KcJ4ImnPzPrFPawElrh7VVR5RBJJnRhpUWb2mbsPM7PPgTNrTqs2s72Ae91dE96JJJiZ7QIcRPAF4k13/yriSCIJoRl7paXVjLNaGz8vjLt/SFCNEZEEMrOTgeeAHgSHkp41s1OiTSWSGKrESIsys/8C7xIMJnTgyfCpnwJZ7v7bqLKJZAIz+xLY392Xh4+7AhNUBZV0pE6MtCgz6w5cAYwg6MjUetrdd239VCKZo+aQbmPLRNKBTrGWFuXuy4BfRZ1DJIN9ZmZdw4kma84Q/CLaSCKJoUqMJISZ/by+5e7+SGtnERGR9KRKjCTK8Lj7eQRnSnwOqBMjIiItQpUYaRVm1gF4xt2PiDqLiIikB51iLa2lAtg26hAiIpI+dDhJEsLMXiCY+hyCzvJg4KnoEomISLrR4SRJCDMbE/ewEpjr7gujyiMiIulHh5MkIdz9bWAa0BXIB6qjTSQiIulGnRhJCDM7AfgA+GF4m2hmJ0abSkRE0okOJ0lCmNkXwNi4qc+7EUx9vlu0yUREJF2oEiOJtCru/sqoQoiISHrS2UmSKC8Dr5rZE+Hjk8JlIiIiLUKHkyRhzOwoYGz48G13fyHCOCIikmbUiREREZGUpMNJkhBmtppNk93lALnAOnfvGF0qERFJJ+rESEK4e6f4x2Z2JLB3RHFERCQN6XCStBozm+zuQ6POISIi6UGVGEkIM/tR3MMsYDiwPqI4IiKShtSJkUT5Xtz9SmAOcGw0UUREJB3pcJKIiIikJM3YKyIiIilJnRgRERFJSerEiIiISEpSJ0ZEvjMzqzKzyWY2xcyeMbN2W7Gth83suPD+/Wa2SwNtx5pZs+cdMrM5Zpbf1OV12qxt5mtdbWaXNjejiDSdOjEisjU2uPtQdx8ClAPnxD9pZlnfZaPufoa7f91Ak7Fo8kSRjKdOjIi0lHeA7cMqyZtm9jjwpZllmdlfzWySmX1hZmcDWOAOM/vazF4CetRsyMwmmNmI8P7hZvapmX1uZuPNbABBZ+lXYRVoPzMrMLN/ha8xycz2CdftbmbjzOwzM/s7my6FsUVm9h8z+8TMvjKzs+o8d1OYZbyZFYTLtjOzV8N13jGznVrkpykijdI8MSKy1cwsGzgCeDVctCcwxN1nhx2BVe4+0szygPfMbBwwDBgE7AoUAl8DD9bZbgFwHzAm3FY3d19uZvcAa939xrDd48DN7v6umfUDXgN2Bq4C3nX3a8zse0CtTskWnBa+Rltgkpn9y92XAe2BT939EjO7Mtz2BcC9wDnuPt3M9gLuAg78Dj9GEWkmdWJEZGu0NbPJ4f13gAcIDvN85O6z/3979/NiYxTHcfz9EVGMiZ2FhQWJwsLCbAYlxW6UkllaWZj/AX+ElKWS1FhIGQtxN2xM+bmxUBY2YkRjVhyL51xzu93p3jJDT71fq6dzz3nO91ndb+d8n+fU9pPAgW69CzAO7AYmgVullJ/AxySPBtz/CNDp3quU8mWFOE4A+5I/Cy1bk4zVOc7UsfeTLIzwTDNJpur1zhrrZ+AXcLu23wRmk2ypz3unZ+6NI8whaRWYxEj6G0v952HVP/PF3ibgUillrq/faWDY1zYzQh9otsYnSilLA2IZ+YueSY7RJEQTpZQfSR4Dm1boXuq8Xz0TTPo/rImRtNbmgItJNgAk2ZNkM9ABztWamR3A8QFjnwJHk+yqY7fX9u/AWE+/hzRbO9R+h+plB5iubaeAbUNiHQcWagKzl2YlqGsd0F1NOk+zTfUNeJ/kbJ0jSQ4OmUPSKjGJkbTWbtDUu8wneQ1cp1kFvgu8A14B14An/QNLKZ9o6lhmk7xgeTvnHjDVLewFZoDDtXD4LctvSV0GJpPM02xrfRgS6wNgfZKXwFXgWc9vi8D+JM9pal6u1PZp4EKN7w2eESb9M56dJEmSWsmVGEmS1EomMZIkqZVMYiRJUiuZxEiSpFYyiZEkSa1kEiNJklrJJEaSJLXSb/AzaQB5On6fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_confusion_matrix(sgd, test['wine_variant'], predicted, xticks_rotation=-90, normalize='true', ax=ax)\n",
    "# how often label are confused with each other \n",
    "# the diagonal are the right label = true label are same as predited "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-information",
   "metadata": {},
   "source": [
    "The highest proportion of misclassification are often occure with Pinot Noir, Cabenet Sauvignon, and Riesling. Here are the new label we would add to improve the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "sporting-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train.copy()\n",
    "test1 = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "academic-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train1['wine_variant'].isin(['Sauignon Blanc', 'Riesling'])\n",
    "n = test1['wine_variant'].isin(['Sauignon Blanc', 'Riesling'])\n",
    "train1['wine_variant'] = train1['wine_variant'].mask(m, 'Sauignon Blanc/Riesling')\n",
    "test1['wine_variant'] = test1['wine_variant'].mask(n, 'Sauignon Blanc/Riesling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "emotional-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train1['wine_variant'].isin(['Syrah', 'Cabernet Sauvignon'])\n",
    "n = test1['wine_variant'].isin(['Syrah', 'Cabernet Sauvignon'])\n",
    "train1['wine_variant'] = train1['wine_variant'].mask(m, 'Syrah/Cabernet Sauvignon')\n",
    "test1['wine_variant'] = test1['wine_variant'].mask(n, 'Syrah/Cabernet Sauvignon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "innovative-monitoring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Syrah/Cabernet Sauvignon    43938\n",
       "Pinot Noir                  38471\n",
       "Chardonnay                  19443\n",
       "Sauignon Blanc/Riesling      9683\n",
       "Zinfandel                    8327\n",
       "Merlot                       5522\n",
       "Sauvignon Blanc              5113\n",
       "Name: wine_variant, dtype: int64"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see all the topics \n",
    "train1[\"wine_variant\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "compact-antibody",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Syrah/Cabernet Sauvignon    10984\n",
       "Pinot Noir                   9618\n",
       "Chardonnay                   4861\n",
       "Sauignon Blanc/Riesling      2421\n",
       "Zinfandel                    2082\n",
       "Merlot                       1381\n",
       "Sauvignon Blanc              1278\n",
       "Name: wine_variant, dtype: int64"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1[\"wine_variant\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-acrobat",
   "metadata": {},
   "source": [
    "Now we have less topic for the wine. We will generate the model again to see if we have higher F1 scores. \n",
    "\n",
    "#### Generate Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "weekly-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "              Chardonnay       0.84      0.84      0.84      4861\n",
      "                  Merlot       0.85      0.34      0.48      1381\n",
      "              Pinot Noir       0.80      0.83      0.82      9618\n",
      " Sauignon Blanc/Riesling       0.78      0.81      0.80      2421\n",
      "         Sauvignon Blanc       0.89      0.62      0.73      1278\n",
      "Syrah/Cabernet Sauvignon       0.75      0.87      0.81     10984\n",
      "               Zinfandel       0.92      0.48      0.63      2082\n",
      "\n",
      "                accuracy                           0.79     32625\n",
      "               macro avg       0.83      0.69      0.73     32625\n",
      "            weighted avg       0.80      0.79      0.79     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd_new = make_pipeline(CountVectorizer(analyzer=identity), SGDClassifier())\n",
    "sgd_new.fit(train1[\"tokens\"], train1[\"wine_variant\"])\n",
    "predicted_new = sgd_new.predict(test1[\"tokens\"])\n",
    "print(classification_report(test1[\"wine_variant\"], predicted_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-edwards",
   "metadata": {},
   "source": [
    "--- \n",
    "### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-black",
   "metadata": {},
   "source": [
    "The raw review text for wine are kind of vauge, which cause the model algorithm to be unstable. A few models were generated to find the best model for this wine data set including dummy classifier, bernoulli Naive Bayes, multinoulli, SGD classifier, Tfidf Transformer, and truncated SVD. We found the best baseline model would be SGD classifier and Tfidf transformer. After optimizing the combination of SGD classifier + Tfidf transformer, we found the best parameters to generate the highest F1 accuracy scores of 0.75. \n",
    "\n",
    "However, after looking at the coefficients and labels of the model, we found that the model has been overfitted. There are alot of misclassification for the wine Pinot Noir and Cabernet Sauvignon label. To improve the model, we set the margin of the test set to exclude some of the sample. In addition, we also grouped the labels into different group so we can get a better output. The group we assigned were Sauignon Blanc/Riesling and Syrah/Cabernet Sauvignon since they have a lot of misclassification correlation. We would improve the model just by grouping them together. \n",
    "\n",
    "The final model have an accuracy F1 score of 0.79, which is really good compared to the baseline and optimized models. In addition, the macro average score is 0.73 and the weighted average is 0.79, which are better than the baseline and optimized models. Since the review text for this data set are too similar from labels to labels, it is a good idea to group them together to get better output. I also have tried different set of labels, but these two labels seem to be the best of all. Therefore, it is impossible to have an F1 score of .85 or better. The only way to get an F1 or better is to exclude some of the test sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
