{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "played-claim",
   "metadata": {},
   "source": [
    "# LAB 7: Error analysis\n",
    "\n",
    "Objectives\n",
    "* Construct a  linear text classifier using SGDClassifier\n",
    "* Evaluate its performance and categorize the errors that it makes\n",
    "* Eaxmine model's coefficients and decision function values\n",
    "* Interpret model results using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "isolated-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-shadow",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rural-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\n",
    "    \"s3://ling583/lab7-train.parquet\", storage_options={\"anon\": True}\n",
    ")\n",
    "test = pd.read_parquet(\"s3://ling583/lab7-test.parquet\", storage_options={\"anon\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wired-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_sm\",\n",
    "    exclude=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"],\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp.tokenizer(text)\n",
    "    return [t.norm_ for t in doc if not (t.is_space or t.is_punct or t.like_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adverse-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ready-haiti",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b9cddc4b2e4262a99b49140afdbcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19054 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6891033b80bf47f19ec14700c9ee761f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mp.Pool() as p:\n",
    "    train[\"tokens\"] = pd.Series(p.imap(tokenize, tqdm(train[\"text\"]), chunksize=100))\n",
    "    test[\"tokens\"] = pd.Series(p.imap(tokenize, tqdm(test[\"text\"]), chunksize=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-fields",
   "metadata": {},
   "source": [
    "The labels are: GPOL = domestic politics, GSPO = sports, GVIO = war/civil war, GJOB = labor issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enhanced-template",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPOL    7410\n",
       "GSPO    5639\n",
       "GVIO    3712\n",
       "GJOB    2293\n",
       "Name: topics, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"topics\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-recipient",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reduced-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "worldwide-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        GJOB       0.93      0.96      0.94       573\n",
      "        GPOL       0.94      0.94      0.94      1853\n",
      "        GSPO       0.99      0.99      0.99      1410\n",
      "        GVIO       0.91      0.89      0.90       928\n",
      "\n",
      "    accuracy                           0.95      4764\n",
      "   macro avg       0.94      0.95      0.94      4764\n",
      "weighted avg       0.95      0.95      0.95      4764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = make_pipeline(CountVectorizer(analyzer=identity), SGDClassifier())\n",
    "baseline.fit(train[\"tokens\"], train[\"topics\"])\n",
    "base_predicted = baseline.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"topics\"], base_predicted))\n",
    "\n",
    "# macro average is the average of the 4 numbers of f1 \n",
    "# weighted average is the largest number of correctly labelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-wrestling",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Hyperparameter search\n",
    "\n",
    "Find an optimal set of hyperparameters for a Tfidf+SGDClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "altered-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "from logger import log_search\n",
    "from scipy.stats.distributions import loguniform, randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "electronic-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "special-mainland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37123</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.62 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:37123' processes=4 threads=4, memory=16.62 GB>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:37123\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "boxed-worry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'lab-7' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"lab-7\")\n",
    "sgd = make_pipeline(\n",
    "    CountVectorizer(analyzer=identity), TfidfTransformer(), SGDClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fourth-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 1.46 s, total: 11.8 s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    sgd,\n",
    "    {\n",
    "        \"countvectorizer__min_df\": randint(1, 20),\n",
    "        \"countvectorizer__max_df\": uniform(0.5, 0.5),\n",
    "        \"tfidftransformer__use_idf\": [True, False],\n",
    "        \"sgdclassifier__alpha\": loguniform(1e-6, 1e-2),\n",
    "    },\n",
    "    n_iter=50,\n",
    "    scoring=\"f1_macro\",\n",
    ")\n",
    "search.fit(train[\"tokens\"], train[\"topics\"])\n",
    "log_search(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-scale",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare optimized model to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "broadband-bleeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        GJOB       0.97      0.94      0.95       573\n",
      "        GPOL       0.94      0.96      0.95      1853\n",
      "        GSPO       1.00      0.99      1.00      1410\n",
      "        GVIO       0.92      0.90      0.91       928\n",
      "\n",
      "    accuracy                           0.96      4764\n",
      "   macro avg       0.96      0.95      0.95      4764\n",
      "weighted avg       0.96      0.96      0.96      4764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = make_pipeline(\n",
    "    CountVectorizer(analyzer=identity, min_df=5, max_df=.75),\n",
    "    TfidfTransformer(use_idf=True),\n",
    "    SGDClassifier(alpha=1e-4),\n",
    ")\n",
    "sgd.fit(train[\"tokens\"], train[\"topics\"])\n",
    "predicted = sgd.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"topics\"], predicted))\n",
    "\n",
    "# better result as before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "opposite-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_f1 = f1_score(test[\"topics\"], base_predicted, average=\"macro\")\n",
    "sgd_f1 = f1_score(test[\"topics\"], predicted, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "interim-backing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9438637478338561, 0.9543250517483548, 0.010461303914498732)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_f1, sgd_f1, sgd_f1 - base_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "valuable-turkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18635558147944903"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sgd_f1 - base_f1) / (1 - base_f1)\n",
    "\n",
    "# our optimzed model is better than the base one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "everyday-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom_test, wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "neither-rover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 41, 4634)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare if two classifier give different answers\n",
    "# counting how many right/wrong\n",
    "diff = (predicted == test[\"topics\"]).astype(int) - (\n",
    "    base_predicted == test[\"topics\"]\n",
    ").astype(int)\n",
    "sum(diff == 1), sum(diff == -1), sum(diff == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "killing-country",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5460961941914623e-05"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binomial test \n",
    "# null hypothesis is 50/50 chance of getting right/wrong \n",
    "binom_test([sum(diff == 1), sum(diff == -1)], alternative=\"greater\")\n",
    "\n",
    "# the test prove that a diviation from 50/50 is very small = smaller than p-value = significant test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "underlying-weekly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=5829.5, pvalue=1.2775403266405453e-05)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wil coxon test, the sign of the predicted values \n",
    "wilcoxon(diff, alternative=\"greater\")\n",
    "\n",
    "# significant = the signs are right "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-uganda",
   "metadata": {},
   "source": [
    "**TO DO:** Summarize your results: how much better is the optimized model? Is it significantly better than the baseline?\n",
    "\n",
    "The optimized model are clearly better than the base model. The optimized model has a higher accuracy score of .96 compared to .95 in the base model. In addition, macro average f1 score from the optimized model is also higher than the base model. This indicate that the overall average of the four factors are higher in the optimized model. Although, optimized model is significant better than the base model, the scores are not too different. In addition, the binomial test and wilcoxon test also has really small p-value which proved that the optimzed model is significant better than the base model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-combining",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "moral-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "twelve-scheme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        GJOB       0.97      0.94      0.95       573\n",
      "        GPOL       0.94      0.97      0.95      1853\n",
      "        GSPO       1.00      0.99      1.00      1410\n",
      "        GVIO       0.93      0.90      0.91       928\n",
      "\n",
      "    accuracy                           0.96      4764\n",
      "   macro avg       0.96      0.95      0.95      4764\n",
      "weighted avg       0.96      0.96      0.96      4764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feed in raw text instead of tokenized text and use tokenizes = tokenize parameter \n",
    "# get the same result \n",
    "sgd = make_pipeline(\n",
    "    CountVectorizer(preprocessor=identity, tokenizer=tokenize, min_df=5, max_df=0.75),\n",
    "    TfidfTransformer(use_idf=True),\n",
    "    SGDClassifier(alpha=1e-4),\n",
    ")\n",
    "sgd.fit(train[\"text\"], train[\"topics\"])\n",
    "predicted = sgd.predict(test[\"text\"])\n",
    "print(classification_report(test[\"topics\"], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bottom-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudpickle.dump(sgd, open(\"sgd.model\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
