{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seven-spokesman",
   "metadata": {},
   "source": [
    "# LAB 9: Sentiment analysis using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exclusive-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-paintball",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atlantic-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-casino",
   "metadata": {},
   "source": [
    "Connect to the GPU (training RNNs without a GPU is veeery slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "everyday-virus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA Tesla T4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-failing",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "primary-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"s3://ling583/sentiment.parquet\", storage_options={\"anon\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pressed-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test set \n",
    "train, test = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"sentiment\"], random_state=619\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-charter",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Because every problem and every model is a little bit different, pytorch (unlike scikit-learn) doesn't have built-in `fit` and `predict` methods. We need to define them ourselves here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-armstrong",
   "metadata": {},
   "source": [
    "This function gathers up a batch of training examples, encodes them, and sends them to the GPU for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "significant-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take review, process, and transfer to GPU processing\n",
    "def collate_batch(batch):\n",
    "    labels, texts = zip(*batch)\n",
    "    texts = [\n",
    "        [vocab[token] for token in [\"<s>\"] + tokenize(t) + [\"</s>\"]] for t in texts\n",
    "    ]\n",
    "    texts = [torch.tensor(t, dtype=torch.int64) for t in texts]\n",
    "    texts = pad_sequence(texts, padding_value=vocab[\"<pad>\"])\n",
    "    labels = torch.tensor([label_vocab[l] for l in labels], dtype=torch.int64)\n",
    "    return labels.to(device), texts.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-vienna",
   "metadata": {},
   "source": [
    "This one applies the model to some test data, for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "joined-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            results = []\n",
    "            for _, text in dataloader:\n",
    "                results.extend(model(text))\n",
    "    return results\n",
    "\n",
    "\n",
    "def predict(dataloader):\n",
    "    predicted = decision_function(dataloader)\n",
    "    return [label_vocab.itos[p.argmax()] for p in predicted]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-progressive",
   "metadata": {},
   "source": [
    "And this is the important part: the function that actually trains the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "familiar-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs=5, batch_size=64, wd=None, clip=None):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if wd:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), weight_decay=wd)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "    t, v = train_test_split(train, test_size=0.1, stratify=train[\"sentiment\"])\n",
    "    train_dataset = list(zip(t[\"sentiment\"], t[\"text\"]))\n",
    "    valid_dataset = list(zip(v[\"sentiment\"], v[\"text\"]))\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        n = 0\n",
    "        for label, text in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                predicted = model(text)\n",
    "                loss = criterion(predicted, label)\n",
    "                correct += (predicted.argmax(1) == label).sum().item()\n",
    "                n += len(label)\n",
    "            scaler.scale(loss).backward()\n",
    "            if clip:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        train_acc = correct / n * 100.0\n",
    "        valid_pred = predict(valid_dataloader)\n",
    "        valid_acc = accuracy_score(v[\"sentiment\"], valid_pred) * 100.0\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch:2d} Time: {elapsed:6.3f}s \"\n",
    "            f\"Train acc: {train_acc:5.3f} Valid acc: {valid_acc:5.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-channel",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Define the model class\n",
    "\n",
    "Okay, most of what's above is mostly [boilerplate](https://en.wikipedia.org/wiki/Boilerplate_code). Now we'll define the specific model and hyperparameter settings that we're using for this task.\n",
    "\n",
    "First, the model architecture. This is a basic [RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network), using [GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit)s are the recurrent units.\n",
    "\n",
    "The hyperparameters of interest are:\n",
    "\n",
    "* `hidden_size`\n",
    "* `embedding_size`\n",
    "* `hidden_layers`\n",
    "* `bidirectional`\n",
    "* `dropout`\n",
    "\n",
    "They control the ability of the model to learn details. Higher values for the first 3, plus setting `bidirectional` to `True`, increase the representational power of the model. That means it can learn more complex patterns and learn them more quickly. If these values are set too high, though, then the model can learn *too* well--it will simply memorize the training data and you'll get overfitting. The last value, dropout, helps control that. Higher values for `dropout` reduce the model's ability to learn and slow down training. The trick is finding a balance among all these settings that maximize learning while minimizing overfitting, which is unfortunately not easy to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "essential-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab,\n",
    "        num_class,\n",
    "        hidden_size,\n",
    "        embedding_dim=128,\n",
    "        hidden_layers=1,\n",
    "        dropout=0.0,\n",
    "        bidirectional=True,\n",
    "    ):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        if not vocab.vectors is None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(\n",
    "                vocab.vectors, freeze=True, padding_idx=vocab[\"<pad>\"]\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(\n",
    "                len(vocab), embedding_dim, padding_idx=vocab[\"<pad>\"]\n",
    "            )\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.embedding.embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=hidden_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        out_size = hidden_size * hidden_layers\n",
    "        if bidirectional:\n",
    "            out_size = out_size * 2\n",
    "        self.fc = nn.Linear(out_size, num_class)\n",
    "\n",
    "    def forward(self, text, lengths=None):\n",
    "        embedded = self.embedding(text)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        out = torch.cat(torch.unbind(hidden), axis=1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-output",
   "metadata": {},
   "source": [
    "Next we set up the vocabulary (this is the step performed by `CountVectorizer` in scikit-learn) using a [basic tokenizer](https://pytorch.org/text/stable/data_utils.html#get-tokenizer) that comes with pytorch. \n",
    "\n",
    "The \"specials\" are vocabulary items that don't correspond to words but are used internally by the model:\n",
    "\n",
    "* `<pad>` : For implementation reasons the documents in a batch all have to be the same length, so we add copies of the pseudo-word `<pad>` to the end of shorter reviews to make them as long as the longest one. \n",
    "* `<s>`, `</s>` : These mark the beginning and end of the reviews.\n",
    "* `<unk>` : Unknown words (i.e., words which are used in the test data that didn't get seen in the training data) get replaced with `<unk>`\n",
    "\n",
    "There's one adjustable parameter here: raising the value of `min_freq` removes low frequency lexical items (similar to `min_df` in scikit-learn). Increasing it usually doesn't improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "regional-barcelona",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17f4f09e44147728ca6fd2351d2739c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize = get_tokenizer(\"basic_english\")\n",
    "counter = Counter(concat(map(tokenize, tqdm(train[\"text\"]))))\n",
    "vocab = Vocab(\n",
    "    counter,\n",
    "    min_freq=1, # 1 means include everything\n",
    "    specials=(\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"),\n",
    ")\n",
    "label_vocab = Vocab(Counter(train[\"sentiment\"]), specials=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-instrumentation",
   "metadata": {},
   "source": [
    "Now we instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "continuing-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel(\n",
    "    vocab,\n",
    "    len(label_vocab),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=128,\n",
    "    hidden_layers=2,\n",
    "    dropout=0.0,\n",
    "    bidirectional=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-split",
   "metadata": {},
   "source": [
    "And finally, we train! There are three important settings here:\n",
    "\n",
    "* `epochs` : This is the number of passes over the training data that we make when fitting the model. A crude way to avoid overfitting is to reduce this, which stops before training before the model has converged.\n",
    "* `batch_size` : This is the number of reviews that get processed at once during training. In general, increasing `batch_size` makes the program run faster (since it lets us take better advantage of the GPU) but may require more epochs to converge. Setting `batch_size` too high can overload the GPUs memory and lead to a crash. The effects of changing `batch_size` on the final results are hard to predict, but it can make a big difference.\n",
    "* `wd` : This is the \"[weight decay](https://www.fast.ai/2018/07/02/adam-weight-decay/)\" parameter. Setting this to a value other than `None` regularizes the model and can reduce overfitting (similar to setting `alpha` for `SGDClassifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "proof-battery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6bfa1405ed4230b56ead9c1970bf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Time: 74.916s Train acc: 80.969 Valid acc: 87.300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98abd8821f2447ea92a0fcb6475176a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 Time: 73.924s Train acc: 90.478 Valid acc: 88.800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84851ea6fe541398310649de96da5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3 Time: 78.823s Train acc: 93.081 Valid acc: 89.725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652a2ff779aa4c5bb3f6c64edc25b6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 Time: 77.448s Train acc: 95.389 Valid acc: 89.650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33aa7fdbd0fb47b4b87b02140d47393d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5 Time: 75.946s Train acc: 97.267 Valid acc: 89.775\n"
     ]
    }
   ],
   "source": [
    "fit(epochs=5, batch_size=64, wd=None)\n",
    "# if acc started to go down, it's starting to be underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "informed-cursor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.010 F1 = 87.104\n"
     ]
    }
   ],
   "source": [
    "# accuracy of the train set is increasing each time \n",
    "# accuracy of the validation set is also increasing each time \n",
    "test_dataset = list(zip(test[\"sentiment\"], test[\"text\"]))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_predicted = predict(test_dataloader)\n",
    "acc = 100 * accuracy_score(test[\"sentiment\"], test_predicted)\n",
    "f1 = 100 * f1_score(test[\"sentiment\"], test_predicted, average=\"macro\")\n",
    "print(f\"Accuracy = {acc:.3f} F1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-breeding",
   "metadata": {},
   "source": [
    "The accuracy and F1 score here is really good. But we will try different parameters to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "scheduled-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel(\n",
    "    vocab,\n",
    "    len(label_vocab),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=128,\n",
    "    hidden_layers=2,\n",
    "    dropout=0.5, # change 0.0 to 0.5 = after each batch, half of the network will turn off , only half of the model get adjusted, different half each time  \n",
    "    bidirectional=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "filled-cleaning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d792a057fd7476487b5332daedeed41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Time: 90.326s Train acc: 80.433 Valid acc: 88.825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fc17adc01b436da20ce0ea6da43762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 Time: 90.334s Train acc: 90.053 Valid acc: 88.850\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307e50f877124de5a3d23a3e72a0c696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3 Time: 92.697s Train acc: 92.608 Valid acc: 89.625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f850971deb6340528fc5e636d6ba530e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 Time: 90.942s Train acc: 94.561 Valid acc: 89.200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5893289dc2f42388c0df338ee3bd35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5 Time: 90.266s Train acc: 96.375 Valid acc: 89.575\n"
     ]
    }
   ],
   "source": [
    "fit(epochs=5, batch_size=64, wd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "detailed-history",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.410 F1 = 87.639\n"
     ]
    }
   ],
   "source": [
    "test_dataset = list(zip(test[\"sentiment\"], test[\"text\"]))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_predicted = predict(test_dataloader)\n",
    "acc = 100 * accuracy_score(test[\"sentiment\"], test_predicted)\n",
    "f1 = 100 * f1_score(test[\"sentiment\"], test_predicted, average=\"macro\")\n",
    "print(f\"Accuracy = {acc:.3f} F1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "commercial-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun the model to restart the epic accuracy \n",
    "model = TextClassificationModel(\n",
    "    vocab,\n",
    "    len(label_vocab),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=128,\n",
    "    hidden_layers=2,\n",
    "    dropout=0.5, \n",
    "    bidirectional=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "affected-karma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac6071ed2b645ee9d11269ef7fd2331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Time: 89.223s Train acc: 79.044 Valid acc: 87.125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa517e075cca45ecbc37c269fec2f8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 Time: 91.005s Train acc: 89.494 Valid acc: 90.400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb43a6ea591f4aa8ac98e9f7b4b8eb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3 Time: 89.895s Train acc: 92.339 Valid acc: 90.275\n"
     ]
    }
   ],
   "source": [
    "# if the number start to drop out after a certain epic, we can change the number of epochs to that number to avoid getting lower accuracy score \n",
    "fit(epochs=3, batch_size=64, wd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "happy-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.860 F1 = 87.906\n"
     ]
    }
   ],
   "source": [
    "test_dataset = list(zip(test[\"sentiment\"], test[\"text\"]))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_predicted = predict(test_dataloader)\n",
    "acc = 100 * accuracy_score(test[\"sentiment\"], test_predicted)\n",
    "f1 = 100 * f1_score(test[\"sentiment\"], test_predicted, average=\"macro\")\n",
    "print(f\"Accuracy = {acc:.3f} F1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-indonesia",
   "metadata": {},
   "source": [
    "### Run the Model using glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "warming-plant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c7c9d3b8c345bba686914d3e599121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize = get_tokenizer(\"basic_english\")\n",
    "counter = Counter(concat(map(tokenize, tqdm(train[\"text\"]))))\n",
    "vocab = Vocab(\n",
    "    counter,\n",
    "    min_freq=1, # 1 means include everything\n",
    "    specials=(\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"),\n",
    ")\n",
    "label_vocab = Vocab(Counter(train[\"sentiment\"]), specials=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "living-glance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:43, 5.28MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:21<00:00, 19010.91it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab.load_vectors(\n",
    "    \"glove.6B.200d\", unk_init=lambda t: torch.nn.init.uniform_(t, -1.0, 1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-digit",
   "metadata": {},
   "source": [
    "Now we instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aerial-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel(\n",
    "    vocab,\n",
    "    len(label_vocab),\n",
    "    hidden_size=256,\n",
    "    embedding_dim=128,\n",
    "    hidden_layers=2,\n",
    "    dropout=0.5, \n",
    "    bidirectional=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "declared-arthur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727d9324bce0446b93632555165c5ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Time: 89.936s Train acc: 79.903 Valid acc: 88.875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f1618403644983b7ad4e52b207e3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 Time: 94.186s Train acc: 89.958 Valid acc: 90.325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404772b4c1c54f529568299acc44dd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3 Time: 92.789s Train acc: 92.075 Valid acc: 90.925\n"
     ]
    }
   ],
   "source": [
    "fit(epochs=3, batch_size=64, wd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "indie-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.000 F1 = 87.569\n"
     ]
    }
   ],
   "source": [
    "test_dataset = list(zip(test[\"sentiment\"], test[\"text\"]))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_predicted = predict(test_dataloader)\n",
    "acc = 100 * accuracy_score(test[\"sentiment\"], test_predicted)\n",
    "f1 = 100 * f1_score(test[\"sentiment\"], test_predicted, average=\"macro\")\n",
    "print(f\"Accuracy = {acc:.3f} F1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "numeric-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cd0b9df6954a419520d682d8b3bbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Time: 91.334s Train acc: 93.800 Valid acc: 93.700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0085cc992a543a58f374e3647834b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 Time: 90.889s Train acc: 95.528 Valid acc: 93.475\n"
     ]
    }
   ],
   "source": [
    "fit(epochs=2, batch_size=64, wd=None) # add 2 more epoch to see what a 5 epoch would look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "final-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.000 F1 = 87.569\n"
     ]
    }
   ],
   "source": [
    "test_dataset = list(zip(test[\"sentiment\"], test[\"text\"]))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_predicted = predict(test_dataloader)\n",
    "acc = 100 * accuracy_score(test[\"sentiment\"], test_predicted)\n",
    "f1 = 100 * f1_score(test[\"sentiment\"], test_predicted, average=\"macro\")\n",
    "print(f\"Accuracy = {acc:.3f} F1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-surge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
